---
output:
    github_document:
    pandoc_args: --webtex
always_allow_html: true
---


<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "imgs/",
  out.width = "100%"
)

knitr::opts_chunk$set(echo = TRUE)
```


<!-- badges: start -->
![forthebadge](https://img.shields.io/badge/GEMM-Building-orange)
![forthebadge](https://forthebadge.com/images/badges/built-with-science.svg)
<!-- badges: end -->

# Recupera√ß√£o de Genomas apartir de metagenomas- Binning! <img src="imgs/1.png" align="right" width = "120px"/>
**Developer: MsC. Kelly Hidalgo**

Pipeline para reconstru√ß√£o de genomas a partir de metagenomas, usando v√°rias ferramentas de *binning* e *DAS tools* para integrar os resultados dos algoritmos de binning para calcular um conjunto otimizado e n√£o redundante de MAGs (*Metagenome Assembled Genomes*) de uma √∫nica montagem.

---

## Introdu√ß√£o

### Sequenciamento massivo de metagenomas

O sequenciamento massivo ou em larga escala de metagenomas, compreende a extra√ß√£o do DNA total de uma amostra de qualquer ambiente ou tipo de amostra (i.e. solo, agua, feces, esponjas marinas, tecidos vegetais, etc) e o sequenciamento por uma t√©cnica chamada *shotgun*. Onde o DNA total √© fragmentado em muitos pedacinhos, os quais s√£o sequenciados aleatoriamente, obtendo assim as chamadas *reads*, que s√£o sequ√™ncias pequenas. 

<img src="imgs/seq1.png" align="center" width = "100%"/>

O *binning* √© uma t√©cnica bioinform√°tica para reconstruir genomas a partir de metagenomas. O processamento inclui as seguintes fases: **1) Sequenciamento de metagenomas**, do qual vai se obter reads; **2) Controle de qualidade das reads**, usando ferramentas como *Trimmomatic* s√£o filtradas as reads de baixa qualidade; **3) Montagem das reads**, dos quais v√£o se obter contigs ou scafolds; **4) Mapeamento das reads**, isto com o objetivo de saber a abund√¢ncia e o origem de cada read (informa√ß√£o necess√°ria para a seguinte etapa); **5)** ***Binning***, usando diferentes ferramentas/algoritmos (i.e. *Metabat2, MaxBin, CONCOCT, BinSanity*) que clusterizam os contigs/scalfolds baseado em diferentes caracter√≠sticas similares tais como: n√≠veis de cobertura de cada contig/scalfold, genes marcadores de c√≥pia √∫nica, perfil de frequ√™ncias de tetranucleot√≠deos, formando os genomas reconstruidos tamb√©m conhecidos como *MAGs* ou *Bins*; **6) Controle de qualidade**, compreende o uso de ferramentas bioinform√°ticas (i.e. *CheckM, DAStool*) para √° analise da qualidade de cada um dos bins gerados, em √≠tems como completude e contamina√ß√£o; **7) Anota√ß√£o taxon√¥mica** e **8) Anota√ß√£o funcional**

A continua√ß√£o uma vis√£o geral do processo de **Binning**

<img src="imgs/binning.png" align="center" width = "100%"/>


> ## **O que vai encontrar aqui:**
>
> ### * Controle de qualidade de sequenciamento shotgun 
>
> ### * Montagem de metagenomas 
>
> ### * Mapeamento de reads 
>
> ### * Reconstru√ß√£o de genomas 
>
> ### * Anota√ß√£o taxon√¥mica e funcional de genomas  

---

# Ferramentas bioinform√°ticas

**Antes de come√ßar:** Use o tutorial de [Unix](https://github.com/khidalgo85/Unix) para aprender comandos b√°sicos em bash que ser√£o muito √∫teis para este tutorial.

## Intala√ß√£o Anaconda

√â recomend√°vel instalar Anaconda, pois √© a forma mais f√°cil para instalar as ferramentas bioinform√°ticas necess√°rias pro desenvolvimento deste pipeline. Anaconda √© uma distribui√ß√£o livre e aberta das linguagens *Python* e *R*, utilizada na ci√™ncia de dados e bioinform√°tica. As diferente vers√µes dos programas se administram mediante um sinstema de gest√£o chamado *conda*, o qual faz bastante simples instalar, rodar e atualizar programas. [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encontram as instru√ß√µes para a instala√ß√£o de Anaconda. 

Depois de instalado, *Anaconda* e o gestor *Conda*, podram ser criados *ambientes virtuais* par a instala√ß√£o das diferentes ferramentas bioinform√°tica que ser√£o usadas. 

> üá™üá∏ Es recomendable instalar Anaconda, pues es la forma m√°s f√°cil para instalar las herramientas bioinform√°ticas necesarias para el desarrollo de este pipeline. Anaconda es una distribuci√≥n libre y abierta de los lenguajes *Python* y *R*, utilizada en ciencia de datos y bioinform√°tica. Las diferentes versiones de los programas se administran mediante un sistema de gesti√≥n llamado *conda*, el cual hace bastante sencillo instalar, correr y actualizar programas.  [Aqui](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) se encuentran las instrucciones para la instalaci√≥n de Anaconda.
>
> Despu√©s de instalado *Anaconda* y su gestor *Conda*, podran ser creados *ambientes virtuales* para la instalaci√≥n de las diferentes herramientas bioinform√°ticas que ser√°n usadas.

---

# I. Binning

## 0. Organizando os dados

## 0.1. Sequ√™ncias

Em este tutorial ser√° usado um dataset exemplo com quatro amostras. A continua√ß√£o descarregue o dataset:

```
# Crie um diret√≥rio para este tutorail
mkdir binning 
cd binning/
```
Agora dentro de binning crie outro diret√≥rio chamado `00.RawData`, onde vai descarregar o dataset de exemplo para este tutorial

```
mkdir 00.RawData
```

Para descarregar o dataset...
```
curl -L https://figshare.com/ndownloader/articles/19015058/versions/1 -o 00.RawData/dataset.zip
unzip 00.RawData/dataset.zip
rm 00.RawData/dataset.zip
```
Com `ls`voc√™ pode ver o conte√∫do descarregado. 

```
ls 00.RawData
```

Por √∫ltimo "listou" (`ls`) o conte√∫do da pasta `00.RawData`, vai observar que t√™m 4 amostras paired-end (R1 e R2) 
```
Sample1_1.fq.gz Sample1_2.fq.gz Sample2_1.fq.gz Sample2_2.fq.gz Sample3_1.fq.gz Sample3_2.fq.gz Sample4_1.fq.gz Sample4_2.fq.gz Sample5_1.fq.gz Sample5_2.fq.gz Sample6_1.fq.gz Sample6_2.fq.gz
```
√â fortemente recomendado rodar os comandos desde o diret√≥rio base, que neste caso √©: `binning/`


> ## **Nota importante: A maioria dos comandos que encontrar√° a continua√ß√£o, ter√£o um par√¢metro para definir o n√∫mero de n√∫cleos/threads/cpus (`-t/--threads/`) que ser√£o usados para o processamento de cada comando. Coloque o n√∫mero de n√∫cleos baseado na sua m√°quina o servidor que esteja usando para rodar as an√°lises. Procure n√£o usar todos os n√∫cleos dispon√≠veis.**


## 1. Controle de qualidade

## 1.1. Avalia√ß√£o da qualidade

üáßüá∑ Para a avalia√ß√£o da qualidade ser√° usado o programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que √© uma ferramenta que permite observar graficamente a qualidade das sequencias de Illumina. 

> üá™üá∏ Para la evaluaci√≥n de la calidad ser√° usado el programa [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) que es una herramienta que  permite observar graficamente la calidad de las secuencias de Illumina. 

### 1.1.1. Instala√ß√£o 

Las instru√ß√µes para a instala√ß√£o usando conda se encontram [aqui](https://anaconda.org/bioconda/fastqc). No entanto neste tutorial tamb√©m ser√£o apresentados.

Como j√° foi explicado anteriormente, com conda √© poss√≠vel criar ambientes virtuais para instalar as ferramentas bioinform√°ticas. O primeiro ambiente que ser√° criado se chamar√° **QualityControl**, onde se instalaram os programas relacionados com esse processo.

> üá™üá∏ [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) es una herramienta para evaluar graficamente la calidad de las secuencias de Illumina. 
>
> Las instrucciones para instalaci√≥n usando conda se encuentran [aqui](https://anaconda.org/bioconda/fastqc). Sin embargo aqui en este tutorial tambi√©n ser√°n presentadas
>
> Como ya fue explicado anteriorimente, con conda es posible crear ambientes virutuales para instalar las herramientas bioinform√°ticas. El primer ambiente que ser√° creado se llamar√° **QualityControl**, donde se instalaran los programas relacionados con este proceso.

```
conda create -n QualityControl
```
üáßüá∑ Durante o processo, o sistema perguntar√° se deseja proceder com a crea√ß√£o do ambiente, com as op√ß√µes y/n (sim ou n√£o). Escreva `y` e depois disso o ambiente virutal estar√° criado.

Para instalar as ferramentas dentro do ambiente anteriormente criado, √© necess√°rio ativ√°-lo.

> üá™üá∏ Durante el proceso, el sistema preguntar√° s√≠ desea proceder con la creaci√≥n del ambiente, con las opciones y/n (si o no). Escriba `y` y despu√©s de eso el ambiente virtual estar√° creado.
>
> Para instalar las herramientas dentro del ambiente anteriormente creado, es necesario activarlo

```
conda activate QualityControl
```
üáßüá∑ O ambiente estar√° ativo quando o nome se encontre ao come√ßo da linha do comando, asssim: `(QualityControl) user@server:~/$`.
Posteriormente se procede √† instala√ß√£o do programa:

> üá™üá∏ El ambiente estar√° activo cuando el nombre de √©ste se encuentra en el comienzo de la linea de comando, as√≠: `(QualityControl) user@server:~/$`.
> 
> Posteriormente se procede a la instalaci√≥n del programa:

```
conda install -c bioconda fastqc
```

### 1.1.2. Uso

üáßüá∑ A primeira etapa do processo √© a avalia√ß√£o da qualidade das sequ√™ncias cortas (Illumina paired end) usando *FastQC*, com o objetivo de determianr se √© necess√°rio trimar ou filtrar as sequ√™ncias da baixa qualidade para nos pr√≥ximos pasos. 

Esta etapa √© para identificar principalmente as sequ√™ncias *outlier* com baixa qualidade ($Q<20$)

Ative o ambiente `QualityControl`:

> üá™üá∏ La primera etapa del proceso es la evaluaci√≥n de la calidad de las secuencias cortas (Illumina paired end) usando *FastQC*, con el objetivo de determinar s√≠ es necesario trimar o filtrar las secuencias de baja calidad en los pr√≥ximos pasos. 
>
> √âsta etapa es para identificar principalmente las secuencias *outlier* con baja calidad ($Q<20$).
>
> Active el ambiente `QualityControl`:

```
conda activate QualityControl

## Onde vc est√°?
pwd
```

üáßüá∑ Deve estar em `~/binning/`.. Se esse n√£o √© o resultado del comando `pwd`, use o comando `cd` para chegar no diret√≥rio desejado.

> üá™üá∏ Debe estar em `~/binning/`. Si ese no es el resultado del comando `pwd`, use el comando `cd` para llegar en el directorio base.

Execute  **FastQC**:
```
## Crie um direct√≥rio para salvar o output do FastQC
mkdir 01.FastqcReports
## Run usando 10 threads
fastqc -t 10 00.RawData/* -o 01.FastqcReports/
```

**Sintaxe**
`fastqc [op√ß√µes] input -o output`

üáßüá∑ O comando `fastqc` tem v√°rias op√ß√µes ou par√¢metros, entre eles, escolher o n√∫mero de n√∫cleos da m√°quina para rodar a an√°lise, para este exemplo `-t 10`. O input √© o diret√≥rio que contem as sequ√™ncias `00.RawData/*`, o `*` indica ao sistema que pode analisar todos os arquivos que est√£o dentro desse diret√≥rio. O output, indicado pelo par√¢mtero `-o`, √© o diret√≥rio onde se deseja que sejam guardados os resultados da an√°lise. A continua√ß√£o se encontram uma explica√ß√£o detalhada de cada output gerado.

> üá™üá∏ El comando `fastqc` tiene varias opciones o parametros, entre ellas, escoger el n√∫mero de n√∫cleos de la m√°quina para correr el an√°lisis, para este caso `-t 10`. El input es el directorio que contiene las secuencias `00.RawData/*`, el `*` indica al sistema que puede analizar todos los archivos que est√°n dentro de ese directorio. El output, indicado por el parametro `-o`, es el directorio donde se desea que sean guardados los resultados del an√°lisis. A continuaci√≥n se encuentra una explicaci√≥n detallada de cada output generado.

**Outputs**

üáßüá∑ 

* Reportes html `.html`: Aqui √© poss√≠vel ver toda informa√ß√£o de qualidade graficamente. 

* Zip files `.zip`: Aqui se encontram cada um dos gr√°ficos de maneira separada. **IGNORE**

Descarregue os arquivos `html` e explore no seu *web browser*. 

Observe as estat√≠sticas b√°sicas que se encontram na primeira tabela. Al√≠, voc√™ pode saber quantas sequ√™ncias tem, o tamanho e o %GC. O gr√°fico mais importante para saber a quealidade das leituras, √© o primeiro, *Per base sequence quality*. Este gr√°fico √© um boxplot com a distribui√ß√£o dos valores de qualidade *Phred Score* (eje y) em cada um dos nucleot√≠deos das leituras (eje x). Se consideram sequ√™ncias de excelente qualidade quando o *Phred Score > 30*. √â norla que o pair 2 apresente uma qualidade um pouco inferior ao pair 1.

> üá™üá∏ Observe las estad√≠sticas b√°sicas que se encuentran en la primera tabla. All√≠, ud puede saber cuantas secuencias tiene, el tama√±o y el %GC. El gr√°fico m√°s importante para saber la calidad de las lecturas es el primero, *Per base sequence quality*. Este gr√°fico es un boxblot con la distribuci√≥n de los valores de calidad *Phred Score* (eje y) en cada uno de los nucle√≥tidos de las lecturas (eje x). Se consideran secuencias de excelente calidad cuando el *Phred Score > 30*. Es normal que el pair 2 presente una calidad un poco inferior al pair 1. 


### 1.2. Trimagem

> üá™üá∏ 1.2 Depuraci√≥n


üáßüá∑ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) √© um programa pra filtrar (remover) leituras ou *reads* curtas de baixa qualidade.

Trimmomatic tem v√°rios par√¢metros que podem ser considerados para filtrar leituras com baixa qualidade. No presente tutorial usaremos alguns deles. Se quiser saber que otros par√¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).


> üá™üá∏ [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) es un programa para filtrar (remover) lecturas o *reads* cortas de baja calidad.
>
> Trimmomatic tiene v√°rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

### 1.2.1. Instala√ß√£o 

üáßüá∑ Como se trata de uma ferramenta que participa dentro do processo de control de qualidade, ser√° instalada dentro do ambiente virtual **QualityControl**.

> Como se trata de una herramienta que participa dentro del proceso de control de calidad, ser√° instalada dentro del ambiente virtual **QualityControl**

```
# Si no est√° activado el ambiente
conda activate QualityControl

# Instale Trimmomatic
conda install -c bioconda trimmomatic
```

### 1.2.2. Uso

üáßüá∑ Segundo foi avaliado no controle de qualidade, pode ser necess√°rio filtrar algumas leituras com qualidade baixa.

O programa Trimmomatic tem v√°rios par√¢metros que podem ser considerados para filtrar reads com baixa qualidade. Aqui usaremos alguns. Se quer saber que outros par√¢metros e como funciona cada um deles, consulte o [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Para os dados aqui analizados se usara a seguinte linha de comando:

> üá™üá∏ Seg√∫n fue evaluado en el control de calidad, puede ser necesario filtrar algunas lecturas con calidad baja.
>
> El programa Trimmomatic tiene v√°rios parametros que pueden ser considerados para filtrar lecturas con baja calidad. Aqui usaremos algunos. Si quiere saber que otros parametros y como funciona cada uno de ellos, consulte el [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
> 
> Para los datos aqui analizados se usar√° la siguiente linea de comando:

```
# Activa o ambiente QualityControl
conda activate QualityControl

# Crie uma pasta para salvar as reads limpas
mkdir 02.CleanData

# Crie uma pasta para salvar as reads n√£o pareadas
mkdir unpaired

# Corra Trimmomatic
trimmomatic PE -threads 10 00.RawData/Sample1_1.fastq.gz 00.RawData/Sample1_2.fastq.gz 02.CleanData/Sample1_1_paired.fastq.gz unpaired/Sample1_1_unpaired.fastq.gz 02.CleanData/Sample1_2_paired.fastq.gz unpaired/Sample1_2_unpaired.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:150
```

üáßüá∑ Com o comando anterior voc√™ tem que rodar a linha de comando para cada amostra. Se quiser rodar todas as amostras de maneira autom√¢tica √© poss√≠vel usar um *loop* `for` para executar esta tarefa.

> üá™üá∏ Con el comnado anterior ud tiene que correr esa l√≠nea de comando para cada muestra. Si quiere correr todas las muestras de manera autom√°tica es posible usar un *loop* `for` para ejecutrar esta tarea. 

```
# loop
for i in 00.RawData/*1.fq.gz 
do
BASE=$(basename $i 1.fq.gz)
trimmomatic PE -threads 10 $i  00.RawData/${BASE}2.fq.gz 02.CleanData/${BASE}1_paired.fq.gz unpaired/${BASE}1_unpaired.fq.gz 02.CleanData/${BASE}2_paired.fq.gz unpaired/${BASE}2_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100
done
```

**Sintaxe**
`trimmomatic PE -threads input_forward input_reverse output_forward_paired output_forward_unpaired output_reverse_paired output_reverse_unpaired [op√ß√µes]`

üáßüá∑ O comando anterior tem muitas partes. Primeiro, o nome do comando √© `trimmomatic`, a continua√ß√£o a op√ß√£o `PE` indica para o programa que as sequ√™ncias que ir√£o ser analisadas s√£o de tipo *paired end*. Depois se encontram os inputs, forward (pair1) e reverse (pair2). Depois est√£o os outputs, sendo o primeiro, as sequ√™ncias forward pareadas (limpas) e n√£o pareadas ("descartadas") e depois igual para as sequ√™ncias reverse. Por √∫ltimo se encontram os par√¢metros de filtragem. Para este caso usamos os par√¢metros `SLIDINGWINDOW`, `LEADING` e `TRAILING`. O primeiro de eles, gera uma janela deslizante, que em este caso vai de 4 em 4 bases, c√°lcula a m√©dia do *Phred Score* e se estiver por baixo de 15 essas bases ser√£o cortadas. `LEADING` corta bases do come√ßo da leitura que estejam por debaixo do *threshold* de qualidade, igualmente faz o `TRAILING` mas no final das leituras. `MINLEN` elimina todas as reads com tamanho menor ao informado. Trimmomatic tem muitos mais par√¢metros para customizar, veja no [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).

Durante o processo de Trimmomatic, o programa vai imprimendo na tela a porcentagem de sequ√™ncias que sobreviveram na trimmagem. No caso do dataset exemplo, em todas as amostras sobreviveram mais do 90% das reads. No entanto √© necess√°rio avaliar a qualidade das sequ√™ncias limpas usando novamente FastQC.

> üá™üá∏ El comando anterior tiene muchas partes. Primero, el nombre del comando es `trimmomatic`, a continuaci√≥n la opci√≥n `PE` indica para el programa que las secuencias que ir√°n a ser analizadas son de tipo *paired end*. Despu√©s se encuentran los inputs, forward (pair1) y reverse (pair2). Despu√©s son los outputs, siendo primero las secuencias forward pareadas (limpias) y no pareadas ("descartadas") y despu√©s las secuencias reverse. Por √∫ltimo se encuentran los parametros de filtrado. Para este caso usamos los parametros `SLIDINGWINDOW`, `LEADING` y `TRAILING`. El primero de ellos, genera una ventana deslizante, que en este caso va de 4 en 4 bases, c√°lcula el promedio del *Phred Score* y si est√° por debajo de 15 esas bases son cortadas. `LEADING` corta bases del comienzo de la lectura si est√°n por debajo de *threshold* de calidad, lo mismo hace `TRAILING` pero al final de las lecturas. `MINLEN` elimina todas las lecturas con tama√±o menor al informado. Trimmomatic tiene muchos m√°s par√°metros customizables, revise en el  [manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf).
>
> Durante el proceso de Trimmomatic, e√± programa va ir imprimiendo en la pantalla el porcentaje de secuencias que sobrevivieron a la depuraci√≥n. En el caso del dataset ejemplo, en todas las muestras sobrevivieron mas del 90% de las reads. Sin embargo, es necesario evaluar la calidad de las secuencias limpias usando nuevamente FastQC.

```
fastqc -t 10 02.CleanData/* -o 01.FastqcReports/
```
Descarregue os arquivos `.html` das sequ√™ncias pareadas (i.e. `01.FastqcReports/Sample1_1_paired_fastqc.html` y `01.FastqcReports/Sample1_2_paired_fastqc.html`).

A qualidade das amostras melhoraram sustancialmente, por tanto est√£o prontas para serem usadas nas pr√≥ximas etapas.

Fa√ßa uma tabela com o n√∫mero de sequ√™ncias antes e depois da trimagem para calcular a porcentagem de *reads* que sobreveviveram ao processo.

> üá™üá∏ Haga una tabla con el n√∫mero de secuencias antes y despu√©s de la depuraci√≥n para calcular el porcentaje de *reads* que sobrevivieron al proceso.


### 1.3 Cobertura dos Metagenoma

üáßüá∑ Al√©m de limpar e trimar as sequ√™ncias com baixa qualidade, √© necess√°rio calcular a cobertura dos metagenomas.Este programa usa a redund√¢ncia de reads nos metagenomas para estimar a cobertura m√©dia e prediz a quantidade de sequ√™ncias que s√£o requeridas para atingir o *"nearly complete coverage"*, definida como $‚â•95$%  ou $‚â•99$% de cobertura m√©dia. A ferramenta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) ser√° usada nesta etapa.

> üá™üá∏ Adem√°s de limpiar y *trimar* las secuencias con baja calidad, es necesario calcular la cobertura de los metagenomas. Este programa usa la redundancia de las *reads* en los metagenomas para estimar la cobertura promedio y predice la cantidade de secuencias que son requeridas para conseguir el *"nearly complete coverage"*, definida como $‚â•95$%  o $‚â•99$% de la cobertura promedio. La herramienta [**NonPareil v3.3.3**](https://nonpareil.readthedocs.io/en/latest/) ser√° usada en esta etapa.

### 1.3.1. Instala√ß√£o

üáßüá∑ [NonPareil v3.3.3](https://nonpareil.readthedocs.io/en/latest/) √© uma ferramenta que ser√° usada para o c√°lculo da cobertura dos metagenomas. Devido a incompatibilidades com a vers√£o do Python usado para escrever esta ferramenta, ela ser√° instalada em um ambiente diferente ao de controle de qualidade, chamado **NonPareil**.

> üá™üá∏ [NonPareil](https://nonpareil.readthedocs.io/en/latest/) es una herramienta que ser√° usada para el c√°lculo de la cobertura de los metagenomas. Debido a incompatibilidades con la versi√≥n de Python usado para escribir esta herramienta, ser√° instalada en un ambiente diferente al de control de calidad, llamado **NonPareil**.

```
# Crie o ambiente
conda create -n NonPareil

# Instale NonPareil
conda install -c bioconda nonpareil
```

### 1.3.2. Uso

Como *input* para esta an√°lise s√≥ √© necess√°rio um pair de cada amostra, e deve estar sem compress√£o.


```
# Crie o diret√≥rio pra o output
mkdir 03.NonPareil

# entre no directorio
cd 03.NonPareil

# Copie os pair 1 da pasta 02.CleanData

cp ../02.CleanData/*_1* ./

# Descomprimir 
gunzip -d *

```

üáßüá∑ Agora est√° tudo pronto para rodar a an√°lise, mas antes disso tome-se o tempo para entender o comando que vai usar. Para conhecer que √© cada um dos argumentos, explore o men√∫ de ajuda da ferramenta. 

> üá™üá∏ Ahora est√° todo listo para correr el an√°lisis, pero antes de eso t√≥mese el tiempo para entender el comando que va a usar. Para conocer que es cada uno de los argumentos, explore el men√∫ de ayuda de la herramienta.

```
# Ative o ambiente NonPareil
conda activate NonPareil

# Explore o men√∫ da ferramenta
nonpareil --help

# Comando do NonPareil para cada amostra
 nonpareil -s sample1.fq -T kmer -f fastq -b sample1 -t 10 &

```
No caso, se tiver v√°rias amostras pode usar o seguinte loop para facilitar o processo.

```
for i in ./*.fq
do
BASE=$(basename $i .fq)
nonpareil -s $i -T kmer -f fastq -b $i -t 10
done
```

**Sintaxe**

* `-s`: caminho para o *input* 
* `-T`: algor√≠tmo a ser usado. `kmer` √© recomendado para arquivos `.fastq` e `alignment` √© recomendado para arquivos `.fasta`.
* `-f`: indique aqui o formato do input (p.e. `fastq` ou `fasta`)
* `-b`: prefixo para os *outputs*
* `-t`: n√∫mero de threads

üáßüá∑ Ao terminar esse processo, o programa ter√° criado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada amostra. Descarregue os arquivos `.npo`. Os quais s√£o tabelas delimitadas por tabula√ß√µes com seis colunas. A primeira coluna indica o esfor√ßo de sequenciamento (em n√∫mero de reads), as demais colunas t√™m informa√ß√£o sobre a distribui√ß√£o da redund√¢ncia a determinado esfor√ßo de sequenciamento. Usando os arquivos `.npo` e o R, pode gr√°ficar as curvas de satura√ß√£o. A continua√ß√£o se encontra o script.
Al√©m dos arquivos `.npo` √© necess√°rio criar um arquivo chamado `samples.txt`, o qual deve ter tr√™s colunas (separadas por tabula√ß√µes), a primeira ter√° o nome de cada arquivo `.npo`, a segunda o nome da amostra, e a terceira a cor em formato JSON que vai ser usada para a curva. A continua√ß√£o se encontram uma s√©rie de comandos no bash para gerar o arquivo, no entanto este arquivo pode ser construido em um bloco de notas, ou incluso no excel.

> üá™üá∏ Al terminar este proceso, el programa habr√° creado varios [*outputs*](https://nonpareil.readthedocs.io/en/latest/redundancy.html#output) por cada muestra. Descargue los archivos `.npo`. Los cuales son tablas delimitadas por tabulaciones con seis columnas. La primera columna indica el esfuerzo de secuenciaci√≥n (en n√∫mero de *reads*), las dem√°s columnas tienen informaci√≥n sobre la distribuci√≥n de la redundancia a determinao esfuerzo de secuenciaci√≥n. Usando los archivos `.npo` e R, puede gr√°ficar las curvas de saturaci√≥n. A continuaci√≥n se encuentra el script.
>
> Adem√°s de los archivos `.npo` es necesario crear un archivo llamado `samples.txt`, el cual debe tener tres columnas (separadas por tabulaciones), la primera tendr√° el nombre de cada archivo `.npo`, la segunda el nombre de la muestra, y la tercera el color en formato JSON que va a ser usado para la curva. A continuaci√≥n se encontran una serie de comandos en bash para generar el archivo, sin embargo, este archivo puede ser construido en un bloc de notas, o incluso en excel.

```
# Cria um arquivo com os nomes dos arquivos
ls *.npo > files.txt

# Cria um arquivo com os nomes das amostras

ls *.npo | sed 's/_1_paired.fq.npo//g' > prefix.txt
```

Agora precisa criar uma lista de cores para diferenciar suas amostras no gr√°fico. Use o site [IWantHue](http://medialab.github.io/iwanthue/) para criar uma paleta com o n√∫mero de cores igual ao n√∫merop de amostras. Copie os c√≥digos **HEX json** das cores e coloque dentro de um arquivo (elimine as v√≠rgulas):

> üá™üá∏ Ahora necesita crear una lista de colores para diferencias sus muestras en el gr√°fico. Use el sitio de internet [IWantHue](http://medialab.github.io/iwanthue/) para crear una paleta con el n√∫mero de colores igual al n√∫mero de muestras. Copie los c√≥digos **HEX json** de los colores e coloque dentro de un archivo (elimine las comas):

```
# Crie o arquivo
nano colors.txt

# Copie e cole os c√≥digos
"#c151b6"
"#5eb04d"
"#7d65ce"
"#b5b246"
```
Cree o arquivo final com os t√≠tulos de las columnas e una los tr√™s arquivos gerados anteriormente:

```
echo -e 'File\tName\tCol' > samples.txt

# Unindo os arquivos dentro de samples.txt
paste -d'\t' files.txt prefix.txt colors.txt >> samples.txt
```

Use `less` para explorar o arquivo, ele deve se ver assim:

```
File    Name    Col
Sample1.npo   Sample1   "#c151b6"
Sample2.npo   Sample2   "#5eb04d"
Sample3.npo   Sample3   "#7d65ce"
Sample4.npo   Sample4   "#b5b246"
Sample5.npo   Sample5   "#c75a87"
Sample6.npo   Sample6   "#648ace"
```

Descarregue os arquivos `.npo` e o arquivo `samples.txt`. Usando o seguinte script do R, grafique as curvas de satura√ß√£o. 
**Nota:** todos os arquivos descarregados devem estar dentro de uma pasta s√≥, p.e. `03.NonPareil`.

Descarregue o script [Nonpareil](https://github.com/khidalgo85/Binning/blob/master/nonpareil.R)

```{r, eval=FALSE}
install.packages("Nonpareil") #para instalar o pacote
library(Nonpareil) # ativa o pacote
setwd("~/03.NonPareil") # determina seu diret√≥rio de trabalho (coloque o seu, onde colocou os arquivos .npo e o arquivo samples.txt)

samples <- read.table('samples.txt', sep='\t', header=TRUE, as.is=TRUE); #l√™ o arquivo samples.txt com a informa√ß√£o das amostras

attach(samples);
nps <- Nonpareil.set(File, col=Col, labels=Name, 
                     plot.opts=list(plot.observed=FALSE, 
                                    ylim = c(0, 1.05),
                                    legend.opts = FALSE)) #grafica as curvas

Nonpareil.legend(nps, x.intersp=0.5, y.intersp=0.7, pt.cex=0.5, cex=0.5) #coloca e personaliza a legenda
  
detach(samples);
summary(nps) #mostra o resumo em forma de tabela
```

Vai obter um gr√°fico com as curvas de satura√ß√£o de cada amostra, como este:

<img src="imgs/nonpareil.png" align='center' width="80%">

üáßüá∑ As linhas tracejadas <font color='red'> vermelha </font> e <font color='gray'> cinza </font> representam os *threshold* de 95% e 99% da cobertura m√©dia, respeitivamente. O circulo em cada curva representa a cobertura atual das amostras, o ideal √© que esteja por cima do primeiro *threshold*. As curvas tamb√©m apresentam a estima√ß√£o de quanto esfor√ßo de sequenciamento √© necess√°rio (zetas no eixo x). Devido a que se trata de um dataset exemplo que foi obtido apartir de um subsample aleatorio de um conjunto de dados, a maioria das amostras n√£o conseguem uma boa cobertura. As curvas reais para as amostras originais se apresentam a continua√ß√£o:

> üá™üá∏ Las l√≠neas punteadas <font color='red'> roja </font> y <font color='gray'> gris </font> representam los *threshold* de 95% y 99% de cobertura promedio, respectivamente. El c√≠rculo en cada curva representa la cobertura actual de las muestras, lo ideal es que est√©n por encima del primer *threshold*. Las curvas tambi√©n presentan la estimaci√≥n de cuanto esfuerzo de secuenciaci√≥n es necesario (flechas en el eje x). Debido a que se trata de un dataset ejemplo que fue obtenido a partir de un subsample aleatorio de un conjunto de datos, la mayoria de las muestras no consiguen una buena cobertura. Las curvas reales para las muestras originais se presentan a continuaci√≥n:

<img src="imgs/realnonpareil.png" align='center' width="80%">

### 1.4. An√°lise de Dist√¢ncias MinHash

üáßüá∑ Ap√≥s obter as sequ√™ncias limpas, de boa qualidade, e determinar a cobertura dos metagenomas, √© poss√≠vel fazer a montagem. No entanto, pode ser inclu√≠do um passo extra antes da montagem e √© verificar a similaridade dos datasets para determinar se pode ser usada a abordagem de *co-assembly*, onde s√£o misturadas as *reads* de v√°rios metagenomas para gerar os contigs. O programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa uma t√©cnica chamada redu√ß√£o de dimensionalidad *MinHash* que avalia as dist√¢ncias um a um entre os datasets. 

> üá™üá∏ Despu√©s de obtener las secuencias limpias, de buena calidad, y determinar la cobertura de los metagenomas, es posible hacer el montaje. Sin embargo, puede ser inclu√≠do un paso extra antes del montaje y es verificar la similaridade de los datasets para determinar si puede ser usado el abordaje de *co-assembly*, donde son mezcladas las *reads* de varios metagenomas para generar los contigs. El programa [**Mash v2.3**](https://mash.readthedocs.io/en/latest/) usa una t√©cnica llamada reducci√≥n de dimensionalidad *MinHash* que evalua las distancias un a un entre los datasets.

### 1.4.1. Instala√ß√£o

üáßüá∑ [Mash v2.3](https://mash.readthedocs.io/en/latest/) √© uma ferramenta que usa a t√©cnica de redu√ß√£o da dimensionalidade *MinHash* para calcular as dist√¢ncias um a um entre os datasets, assim, √© poss√≠vel determinar se os metagenomas s√£o similares ou n√£o para serem montados usando *co-assembly*. 

üáßüá∑ Por ser considerada uma ferramenta que participa no processo de assembly, ser√° instalada dentro de um ambiente virtual chamado **Assembly**.

> üá™üá∏ [Mash](https://mash.readthedocs.io/en/latest/) es una herramienta que usa la t√©cnica de reducci√≥n de dimensionalidad *MinHash* para calcular las distancias un a un entre los datasets, as√≠, es posible determinar si los metagenomas son similares o no para ser ensamblados usando *co-assembly*.
>
> üá™üá∏ Por ser considera una herramienta que participa en el proceso de ensamble, ser√° instalada dentro de un ambiente virtual llamado **Assebly**.

```
# Crie o ambiente virtual
conda create -n Assembly

# Instale Mash
conda install -c bioconda mash
```

### 1.4.2. Uso

```
## Crie uma pasta para o output
mkdir 04.MinHash
```

üáßüá∑  O primeiro paso √© concatenar os reads 1 e 2, e armazenar eles na nova pasta criada `04.MinHash/`.

**Nota:** Se voc√™ trimou suas sequ√™ncias, deve usar os arquivos gerados pelo **Trimmomatic** na pasta `02.CleanData`, se pelo contr√°rio suas sequ√™ncias estavam de boa qualidade e n√£o foi necess√°rio trimar, use os arquivos originais, que est√£o dentro da pasta `00.RawData/`.

> üá™üá∏ 
>
>**Nota:** Si usted filtr√≥ sus secuencias, debe usar los archivos generados por **Trimmomatic** en el directorio `02.CleanData`, si por el contrario sus secuencias estaban de buena calidade y no fue necesario filtrar, use los archivos originales, que est√°n dentro de la carpeta `00.RawData`.

```
for i in 02.CleanData/*_1_paired.fq.gz
do
BASE=$(basename $i _1_paired.fq.gz)
cat $i 02.CleanData/${BASE}_2_paired.fastq.gz > 04.MinHash/${BASE}.fq
done
```

üáßüá∑ Depois ser√° criado um *sketch* para combinar todas as amostras. Usando `mash info` pode verificar o conte√∫do e, em seguida, estimar as dist√¢ncias par a par:

> üá™üá∏ 
>
> Despu√©s ser√° creado un *sketch* para combinar todas las muestras. Usando `mash info` puede verificar el contenido y, en seguida, estimar las distancias par a par:

```
mash sketch -o 04.MinHash/reference 04.MinHash/sample1.fq 04.MinHash/sample2.fq 04.MinHash/sample3.fq 04.MinHash/sample4.fq 04.MinHash/sample5.fq 04.MinHash/sample6.fq

#verifiyng
mash info 04.MinHash/reference.msh
```

**Sintaxe**

`mash sketch -o reference [inputs]`

`mash info reference.msh`

* `sketch`: Comando para criar um *sketch*, combinando todas as amostras, recomendado quando t√™m mais de tr√™s amostras.
* `-o`: caminho pro *output*, criar√° um *sketch* `.msh`.
* `inputs`: liste os inputs (sequencias concatenadas dos pair1 e pair2)
* `info`: pode verificar o conte√∫do do `sketch`
* `reference.msh`: *sketch* criado

Por √∫ltimo, calcule as dist√¢ncias entre cada par de metagenomas usando `mash dist` e salve o resultado no arquivo `distancesOutput.tsv`.

```
mash dist 04.MinHash/reference.msh 04.MinHash/reference.msh -p 6 -t > 04.MinHash/distancesOutputFinal.tsv
```

**Sintaxe**
`mash dist [reference] [query] [options]`

* `dist`: comando para calcular as dist√¢ncias entre cada par de mategenomas, baseado na dist√¢ncia *MinHash*.
* `reference`: aqui pode colocar o *sketch* criado, ou arquivos `.fq`, `fasta`.
* `query`: √≠dem
* `-p`: n√∫mero de threads
* `-t`: indica o tipo de formato de matriz

Descarregue o output (`04.MinHash/distancesOutputFinal.tsv`) e use o seguinte script do R para plotar um heatmap com as dist√¢ncias.

Descarregue o script para graficar o heatmap das distancias [MinHash](https://github.com/khidalgo85/Binning/blob/master/minhash.R)

```{r, eval=FALSE}
setwd("~/04.MinHash/")

# install.packages('dplyr')
library(dplyr)
# install.packages('stringr')
library(stringr)
# install.packages('tidyverse')
library(tidyverse)

data <- read.table("distancesOutputFinal.tsv", comment.char = '', 
                    header = TRUE ) %>% 
  rename(X = X.query) 
  

data$X <- str_remove_all(data$X, "04.MinHash/")
data$X <- str_remove_all(data$X, ".fq")

names <- c("X", data[,1])

colnames(data) <- names

data <- column_to_rownames(data, var="X")

library(pheatmap)


pheatmap(data)
```

Vai obter um heatmap com clusteriza√ß√£o:

<img src="imgs/minhash.png" align='center' width="80%">

Como pode ser observado, se formaram v√°rios clusters, por exemplo Sample5, Sample4 e Sample1, Sample3 e 2, e Sample6 formou um cluster aparte. Assim, poderiam ser feitos dois co-assemblies e um assembly individual. No entanto para facilitar o processo no tutorial, ser√° feito um co-assembly s√≥, com todas as amostras.


## 2. Montagem dos Metagenomas

üáßüá∑ A montagem dos metagenomas √© a etapa mais importante do processo, porque os demais passos para adelante dependen de uma boa montagem. No caso dos metagenomas, se trata de um proceso que n√£o √© para nada trivial, requer um grande esfor√ßo computacional. Por este motivo, ser√£o testados v√°rios par√¢metros, para comparar cada montagem e decidir qual √© o melhor para √°s an√°lises *downstream*. Neste processo ser√° usado o montador [Spades v3.15.3](https://github.com/ablab/spades).

> üá™üá∏ El montaje de los metagenomas es la etapa m√°s importante del proceso, porque los dem√°s pasos para adelante dependen de un buen ensamble. En el caso de los metagenomas, se trata de un proceso que no es para nada trivial, requiere un gran esfuerzo computacional. Por este motivo ser√°n testados varios par√°metros, para comparar cada ensamble y decidir cual es el mejor para los an√°lisis *downstream*. En este proceso ser√° usado el montado [Spades v3.15.3](https://github.com/ablab/spades).

### 2.1. Instala√ß√£o

üáßüá∑ [Spades v3.15.3](https://github.com/ablab/spades) √© um dos montadores de genomas e metagenomas, mais conhecido e com melhores resultados, pode ser usado tanto para leituras curtas como longas. Leia atentamente o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), j√° que este programa tem muitas op√ß√µes diferentes. Spades usa o algor√≠tmo do *Grafo de Bruijn* para a montagem das secu√™ncias.

Siga as seguintes instru√ß√µes para a instala√ß√£o do **Spades** dentro do ambiente virtual *Assembly*.

> üá™üá∏ [Spades v3.15.3](https://github.com/ablab/spades) es uno de los ensambladores de genomas y metagenomas, m√°s conocido y con mejores resultados, puede ser usado tanto para lecturas cortas como largas. Lea atentamente el [manual](http://cab.spbu.ru/files/release3.15.2/manual.html), ya que este programa tiene muchas opciones diferentes. Spades usa el algor√≠tmo del *Grafo de Bruijn* para el montaje de las secuencias. 
>
> Siga las siguientes instrucciones para la instalaci√≥n de **Spades** dentro del ambiente virtual *Assembly*.

```
# Active el ambiente virtual
conda activate Assembly

# Instale Spades
conda install -c bioconda spades
```

### 2.2. Uso

üáßüá∑ Agora √© momento de fazer as montagens. Use o resultado da an√°lisis de dist√¢ncias *MinHash* para decidir como ser√£o feitos as montagens. Amostras muito pr√≥ximas pode fazer *co-assembly*, para amostras distantes √© recomendado montar individualmente. Opcionalmente podem ser usadas as sequ√™ncias no pareadas (sequ√™ncias "descartadas" pelo Trimmomatic). O montador usado neste m√©todo ser√° [Spades](https://github.com/ablab/spades). 

A continua√ß√£o se encontram os comandos se sua **montagem for individual** (n√£o √© o caso das amostras do tutorial, veja mais para frente):

> üá™üá∏ Ahora es el momento de hacer los ensamblajes. Use el resultado del an√°lisis de distancias *MinHash* para decidir como ser√°n hechos los montajes. Muestras muy pr√≥xima puede hacer *co-assembly*, para muestras distantes es recomendado montar individualmente. Opcionalmente pueden ser las secuencias no pareadas (secuencias "descartadas" por Trimmomatic). El montador usado en este m√©todo ser√° [Spades](https://github.com/ablab/spades).

> A continuaci√≥n se encuentran los comandos si su **ensamble fuera individual** (no es el caso de las muestras de este tutorial, vea m√°s adelante)

1. Criar um diret√≥rio para todas as montagens

```
mkdir 05.Assembly
```

2. Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

```
cat unpaired/Sample1_1_unpaired.fq.gz unpaired/Sample1_2_unpaired.fq.gz > unpaired/Sample1_12_unpaired.fq.gz
```

3. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/Sample1/ -1 02.CleanData/Sample1_1_paired.fq.gz -2 02.CleanData/Sample1_2_paired.fq.gz -s unpaired/Sample1_12_unpaired.fq.gz -t 10 -m 100 -k 21,29,39,59,79,99,119
```

**Sintaxe**

* `metaspades.py`: t para montar metagenomas
* `-o`: caminho para diret√≥rio de sa√≠da
* `-1`: caminho para diret√≥rio do pair1
* `-2`: caminho para diret√≥rio do pair2
* `-s`: caminho para diret√≥rio das *reads* no pareadas
* `-t`: n√∫mero de threads
* `-m`: Mem√≥ria em gigas (m√°ximo)
* `-k`: lista de *k-mers*

üáßüá∑ No caso particular das amostras deste tutorial, ser√£o montadas em um co-assembly s√≥. Por tanto siga as seguintes instru√ß√µes:

Se sua montagem for no modo *co-assembly* deve fazer uma etapa anterior, onde vai concatenar todos os pair1 das amostras que ser√£o montadas e todos os pair2 das mesmas. 

> üá™üá∏  En el caso particular, las muestras de este tutorial, ser√°n montadas en un solo co-assembly. Por lo tanto siga las siguientes instrucciones:
> Si su ensamblaje es en el modo *co-assembly* debe hacer una etapa anterior, donde va a concatenar todos los pair1 de las muestras que ser√°n montadas y todos los pair2 de las mismas. 

1. Concatene os pair 1

```
cat 02.CleanData/Sample1_1.fq.gz 02.CleanData/Sample2_1.fq.gz 02.CleanData/Sample3_1.fq.gz 02.CleanData/Sample4_1.fq.gz 02.CleanData/Sample5_1.fq.gz 02.CleanData/Sample6_1.fq.gz > 02.CleanData/Sample_all_1.fq.gz
```

2. Concatene os pair 2

```
cat 02.CleanData/Sample1_2.fq.gz 02.CleanData/Sample2_2.fq.gz 02.CleanData/Sample3_2.fq.gz 02.CleanData/Sample4_2.fq.gz 02.CleanData/Sample5_2.fq.gz 02.CleanData/Sample6_2.fq.gz > 02.CleanData/Sample_all_2.fq.gz
```

3. Se voc√™ quiser usar as *reads* no pareadas (sa√≠da do **Trimmomatic**), deve primeiro concatenarlas em um arquivo s√≥

```
cat unpaired/Sample1_1_unpaired.fq.gz unpaired/Sample1_2_unpaired.fq.gz unpaired/Sample2_1_unpaired.fq.gz unpaired/Sample2_2_unpaired.fq.gz unpaired/Sample3_1_unpaired.fq.gz unpaired/Sample3_2_unpaired.fq.gz unpaired/Sample4_1_unpaired.fq.gz unpaired/Sample4_2_unpaired.fq.gz unpaired/Sample5_1_unpaired.fq.gz unpaired/Sample5_2_unpaired.fq.gz unpaired/Sample6_1_unpaired.fq.gz unpaired/Sample6_2_unpaired.fq.gz > unpaired/Sample_all_unpaired.fq.gz
```

4. Montagem com MetaSpades

```
metaspades.py -o 05.Assembly/ -1 02.CleanData/Sample_all_1.fq.gz -2 02.CleanData/Sample_all_2.fq.gz-s unpaired/Sample_all_unpaired.fq.gz -t 10 -m 100 -k 21,29,39,59,79,99,119
```

**Outputs**

Para conhecer os demais par√¢metros do comando que n√£o foram modificados (usados por *default*), consulte o [manual](http://cab.spbu.ru/files/release3.15.2/manual.html).

* `corrected/`: cont√©m as reads corregidas por **BayesHammer** em `.fastq.gz`

* `scaffolds.fasta`: cont√©m os scaffolds obtidos

* `contigs.fasta`: cont√©m os contigis obtidos

* `assembly_graph_with_scaffolds.gfa`: cont√©m o grafo da montagem en formato GFA 1.0.

* `assembly_graph.fastg`: cont√©m o grafo da montagem em formato FASTG

## 3. Controle de Qualidade das montagens

üáßüá∑ Para avaliar a qualidade das montagens ser√° usada a ferramenta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente o *script* `metaquast.py`, com o qual √© poss√≠vel determinar as principais estat√≠sticas da montagem (i.e. N50, n√∫mero de contigs, tamanho total da montagem, tamanho dos contigs, etc). **Metaquast** gera uma s√©rie de arquivos e reportes onde √© poss√≠vel observar essas estat√≠sticas b√°sicas da montagem. √â uma ferramente muito √∫til para comparar montagens e escolher a melhor pro mesmo conjunto de dados. 

> üá™üá∏ Para evaluar la calidad de los montajes ser√° usada la herramienta [**Quast v5.0.2**](http://quast.sourceforge.net/docs/manual.html) (*QUality ASsesment Tool*), especificamente el *script* `metaquast.py`, con el cual es posible determinar las principales estad√≠sticas del montaje (i.e. N50, n√∫mero de contigs, tama√±o total del montaje, tama√±o de los contigs, etc). **Metaquast** genera una serie de archivos y reportes donde es posible observar esas estad√≠sticas b√°sicas del montaje. Es una herramienta muy √∫til para comparar monatajes y escoger el mejor del mismo conjunto de datos. 

### 3.1. Instala√ß√£o

Crie um novo ambiente virtual, chamado bioinfo, onde se instalar√° **Quast**.

```
# Crie o ambiente
conda create -n bioinfo

# Ative o ambiente bioinfo
conda activate bioinfo

# Instale Quast
conda install -c bioconda quast
```

### 3.2. Uso

üáßüá∑ Se voc√™ tiver v√°rias montagens e quer comparar todas √© necess√°rio trocar os nomes dos assemblies, j√° que eles tem todos o mesmo nome, `contigs.fasta` ou `scaffolds.fasta`. Use o comando `mv` para trocar os nomes. Siga o seguinte exemplo:

> üá™üá∏ Si usted tiene varios ensambles e quiere compararlos es necesario cambiar los nombres de los montajes, ya que todos tienen el mismo nombre, `contigs.fasta` ou `scaffolds.fasta`. Use el comando `mv` para cambiar los nombres. Siga el siguiente ejemplo:


Por exemplo:
```
mv 05.Assembly/sample1/scaffolds.fasta 05.Assembly/sample1/sample1.fasta

mv 05.Assembly/sample45/scaffolds.fasta 05.Assembly/sample45/sample45.fasta
```

Para as amostras deste tutorial n√£o √© necess√°rio trocar os nomes porque s√≥ √© uma montagem:

```
# Crie um diret√≥rio pro output
mkdir 06.AssemblyQuality

# Rode Quast
metaquast.py 05.Assembly/scaffolds.fasta -o 06.AssemblyQuality/ --threads 10
```

**Sintaxis**
`metaquast.py path/to/assembly/contigs.fasta -o path/to/output/`

* Pode colocar v√°rios inputs (montagens) separados por espa√ßo.

**Interpreta√ß√£o dos resultados**

üáßüá∑ A ideia de usar **Metaquast**, a parte de avaliar as estat√≠sticas b√°sicas das montagens, √© comparar varias montagens para escolher a melhor. Por exemplo: entre menor seja o n√∫mero de contigs √© melhor, porque significa que a montagem est√° menos fragmentada. E isso ser√° refletido no tamanho dos contigs que ser√£o maiores. O valor de N50, √© melhor entre maior seja. Al√©m, tamb√©m √© ideal um menor n√∫mero de gaps e Ns. No entanto, estas estat√≠sticas funcionam melhor para genomas que para metagenomas, por se tratar de um conjunto de microrganismos.

> üá™üá∏ La idea de usar **Metaquast**, aparte de evaluar las estid√≠sticas b√°sicas de los montajes, es comparar varios montajes para escoger el mejor. Por ejemplo: entre menor sea el n√∫mero de contigs es mejor, porque significa que el montaje est√° menos fragementado. Y eso se reflejar√° en el tama√±o de los contigs que ser√°n m√°s grandes. El valor de N50, es mejor entre mayor sea. As√≠ mismo, es ideal menor n√∫mero de gaps y Ns. Sin embargo, √©stas estad√≠sticas funcionan mejor para genomas que para metagenomas, por tratarse de un grupo de microorganismos. 

**Outputs**

Explore o diret√≥rio do output usando o comando `ls`.

* `06.AssemblyQuality/report.html`: Este relat√≥rio pode ser aberto em um *web browser* e contem as informa√ß√µes mais relevantes. Como n√∫mero de contigs, tamanho del maior contig, tamanho total da montagem, N50, etc.

> üá™üá∏ `06.AssemblyQuality/report.html`: reporte puede ser abierto en un *web browser* y contiene las informaciones m√°s relevantes. Como n√∫mero de contigs, tama√±o del mayor contig, tama√±o total del montaje, N50, etc.

* `06.AssemblyQuality/report.tex`, `06.AssemblyQuality/report.txt`, `06.AssemblyQuality/report.tsv`, `06.AssemblyQuality/report.pdf`: √© o mesmo relat√≥rio por√©m em diferentes formatos. 

* `06.AssemblyQuality/transposed_report.tsv`, `06.AssemblyQuality/transposed_report.tex`, `06.AssemblyQuality/transposed_report.tex`: Tamb√©m √© o relat√≥rio por√©m em formato tabular. 

* `06.AssemblyQuality/icarus_viewers/contig_size_viewer.html`: Visualizador das contigs

* `06.AssemblyQuality/basis_stats/`: Dentro desta pasta se encontram v√°rios gr√°ficos em formato `.pdf`. 

## 4. Mapping

Agora √© necess√°rio fazer o mapeamento das reads originais dentro do co-assembly para obter informa√ß√µes de cobertura (n√∫mero de vezes que um fragmento √© sequ√™nciado) para cada contig em cada amostra. O programa [Bowtie2](https://github.com/BenLangmead/bowtie2) √© o elegido para esta tarefa. 

> üá™üá∏ Ahora es necesario hacer el mapeamiento de las reds dentro del co-assembly para obtener las informaciones de cobertura (n√∫mero de veces que un fragmento es secuenciado) para cada contig en cada muestra. El programa [Bowtie2](https://github.com/BenLangmead/bowtie2) es el elegido para esta tarea.

### 4.1. Instala√ß√£o

#### 4.1.1. Bowtie2

Crie um novo ambiente virtual, chamado mapping, onde se instalar√° **Bowtie2**.

```
# Crie o ambiente
conda create -n mapping

# Ative o ambiente mapping
conda activate mapping

# Instale Bowtie2
conda install -c bioconda bowtie2
```

#### 4.1.2. Samtools

Para a manipula√ßao dos arquivos usaremos [Samtools](https://github.com/samtools/samtools).

```
# Crie o ambiente
conda create -n mapping

# Ative o ambiente samtools
conda activate samtools

# Instale Samtools
conda install -c bioconda samtools=1.9
```

### 4.2. Uso

Ap√≥s instaldo, crie uma pasta para armazenar a sa√≠da do mapeamento

```
mkdir 07.Mapping
```

O primeiro passo do mapeamento √© criar um √≠ndice de nosso co-assembly

```
# Ative o ambiente mapping
conda activate mapping

 bowtie2-build 05.Assembly/scaffolds.fasta 07.Mapping/final_assembly_DB
```
Agora vamos a mapear as reads das amostras individuais no co-assembly. O processo pode ser feito amostra por amostra, ou podemos usar um loop para fazer todas as amostras ao mesmo tempo. **Cuidado:** Fique atento a nome de suas amostras, e se for necess√°rio modifique o comando, para que se ajuste as suas amostras. 
> üá™üá∏ 
Ahora vamos a mapear las reads de las muestras individuales en el co-assembly. El proceso puede ser hecho muestra por muestra, o puede ser usado un loop para hacer todas las muestras al mismo tiempo.
**Cuidado:** Est√© atento al nombre de sus muestras, y si es necesario modifique el comando, para que se ajuste a sus muestras.

```
for i in 02.CleanData/*_1_paired.fq.gz
do
BASE=$(basename $i _1_paired.fq.gz)
bowtie2 -q -1 $i -2 02.CleanData/${BASE}2_paired.fq.gz -x 07.Mapping/final_assembly_DB -p 10 -S 07.Mapping/${BASE}.sam
done
```

A linha de comando para cada amostra √©:

```
bowtie2 -q -1 02.CleanData/Sample1_1.fq.gz -2 02.CleanData/Sample1_2.fq.gz -x 07.Mapping/final_assembly_DB -p 10 -S 07.Mapping/Sample1.sam
```

Agora √© necess√°rio converter os arquivos `.sam` para `.bam`. Tamb√©m ser√° feito o comando usando um loop

```
# Ative o ambiente Samtools
conda activate samtools

cd 07.Mapping/
for f in *.sam; do filename="${f%%.*}"; samtools view -@ 10 $f -b > ${filename}.bam; done
```
O comando individual √©:

```
samtools view -b -o 06.Mapping/sample1.bam 06.Mapping/sample1.sam
```
Ap√≥s transformar em arquivo `.bam`, devem ser ordenados.

```
for f in *.bam; do filename="${f%%.*}"; samtools sort -@ 10 $f > ${filename}.sorted.bam; done

ls ## conferir que estejam os arquivos

```

E por √∫litmo os arquivos v√£o ser indexados

```
# loop
for f in *.sorted.bam; do filename="${f%%.*}"; samtools index -@ 10 $f > ${filename}.index.bam; done

# Comando individual
samtools index -@ 10 sample1.sorted.bam sample1.index.bam

## para voltar na pasta raiz binning/
cd ..
```
## 6. Binning

Para a reconstru√ß√£o dos genomas, ser√£o usadas v√°rias ferramentas para ter um n√∫mero maior de MAGs recuperados.

> üá™üá∏
Para la reconstrucci√≥n de los genomas, ser√°n usadas varias herramientas para ter un n√∫mero mayor de MAGs recuperados.

### 6.1. MetaBat2

#### 6.1.1 Instala√ß√£o

Para a instala√ß√£o de algumas ferramentas de binning, ser√° criado um ambiente chamado **Binning**.

```
# Cria o ambiente
conda create -n Binning

# Ativa o ambiente Binning
conda activate Binning

# Instala Metabat2
conda install -c bioconda metabat2
```

#### 6.1.2. Uso

Crie uma pasta para armazenar a sa√≠da do processamento em *MetaBat2*

```
mkdir 08.MetaBat2
```

Usando os arquivos ordenados `.sorted.bam` vai gerar um arquivo `.txt` com a informa√ß√£o da cobertura, necess√°ria para a recupera√ß√£o dos genomas.
Como sempre lembrando que precisa ativar o ambiente onde se encontra instalado o *MetaBat2*

> üá™üá∏
Usando los archivos ordenados `.sorted.bam` va a ser generado un archivo `.txt` con la informaci√≥n de cobertura necesaria para la recuperaci√≥n de los genomas. 
Como siempre, recuerde que necesita activar el ambiente donde se encuentra instalado el *Metabat2*

```
jgi_summarize_bam_contig_depths --outputDepth 08.MetaBat2/Depth.txt 07.Mapping/*sorted.bam
```
O *MetaBat2* tem v√°rios par√¢metros para customizar, o tamanho minimo de contig √© o mais comum de ser modificado. Neste pipeline voc√™ vai encontrar tr√™s rodadas com *MetaBat2* com diferentes tamanhos minimos de contigs.

> üá™üá∏
*Metabat2* tiene varios par√°metros para personalizar, el tama√±o m√≠nimo de contig es el m√°s com√∫n de ser modificado. En este pipeline ud va a encontrar tres corridas con *Metabat2* con diferentes tama√±os m√≠nimos de contigs.

**First Trial**

Crie um diret√≥rio dentro da pasta `08.MetaBat2` chamado `01.FirstTrial`

```
mkdir 08.MetaBat2/01.FirstTrial
```

Para este primeiro trial o tamanho minimo de contig ser√° de 1500

```
 metabat2 -i 05.Assembly/scaffolds.fasta -a 08.MetaBat2/Depth.txt -m 1500 -t 10 -o 08.MetaBat2/01.FirstTrial/metabat2_first_
```

Para o segundo trial o tamanho minimo de contig ser√° de 2500, que √© o default da ferramenta, por isso n√£o precisa colocar o flag `-m`.

```
mkdir 08.MetaBat2/02.SecondTrial

metabat2 -i 05.Assembly/scaffolds.fasta -a 08.MetaBat2/Depth.txt -t 10 -o 08.MetaBat2/02.SecondTrial/metabat2_second_
```

Por √∫ltimo, para terceira rodada, ser√£o modificados mais par√¢metros.
Para conhecer todos os par√•metros que podem ser customizados, digite o comando `metabat2 --help`. Com o flag `-m` ou `--minContig`, como j√° foi usado nas rodadas anteriores √© poss√≠vel modificar o tamanho m√≠nimo dos contigs, para este caso ser√° usado 3000. Com o flag `--maxEdges`, pode-se modificar o m√°ximo n√∫mero de *edges* (arestas) por n√≥. Entre maior seja o n√∫mero, o algoritmo √© mais sensitivo. O default √© 200, vai ser usado 500. O flag `--minS` modifica o socre m√≠nimo de cada *edge*, entre maior seja √© mais espec√≠fico. O default √© 60, vai ser usado 80.
Ent√£o o comando √© o seguinte:

> üá™üá∏
Por √∫ltimo, para la tercera corrida, ser√°n modificados m√°s par√°metros. Para conocer todos los par√°metros que puedes ser personalizados, digite el comando `metabat2 --help`. Con el flag `-m` ou `--minContig`, como ya fue usado en las corridas anteriores, es posible modificar el tama√±o m√≠nimo de los contigs, para este caso ser√° usado 3000. Con el flag `--maxEdges`, puede ser modificado el n√∫mero m√°ximo de *edges* (arestas) por nodo. Entre mayor sea el n√∫mero, el algoritmo es m√°s sensitivo. El default es 200, va a ser usado 500 para esta corrida. El flag `--minS` modficia el score m√≠nimo de cada *edge*, entre mayor sea es m√°s espec√≠fico. El default es 60, va ser usado 80. 
Entonces el comando a usar es el siguiente:

```
mkdir 08.MetaBat2/03.ThirdTrial

metabat2 -i 05.Assembly/scaffolds.fasta -a 08.MetaBat2/Depth.txt -t 10 --minContig 3000 --minCV 1.0 --minCVSum 1.0 --minS 80 --maxEdges 500 -o 08.MetaBat2/03.ThirdTrial/metabat2_third_
```

#### 6.2. CONCOCT

A seguinte ferramenta de reconstru√ß√£o de genomas √© [CONCOCT](https://github.com/BinPro/CONCOCT).A qual ser√° instalada usando o Conda.

#### 6.2.1. Instala√ß√£o

Esta ferramenta presenta alguns conflitos com outras ferramentas de binning, principalmente na vers√£o de Python que usa. Por este motivo ser√° criado um ambiente s√≥ para esta ferramenta.

> üá™üá∏
Esta herramienta presenta algunos conflictos con otras herramientas de binning, principalmente en la version de Python que usa. Por este motivo ser√° creado un ambiente para esta herramienta

```
# Crie o ambiente
conda create -n Concoct

# Ative o ambiente
conda activate Concoct

# Instale o CONCOCT
conda install -c bioconda concoct
```

#### 6.2.2. Uso

Crie uma pasta para armazenar os arquivos de sa√≠da do *CONCOCT*

```
mkdir 09.CONCOCT
```
Primeiramente v√£o ser cortados os contigs em partes menores
```
cut_up_fasta.py 05.Assembly/scaffolds.fasta -c 10000 -o 0 --merge_last -b 09.CONCOCT/contigs_10K.bed > 09.CONCOCT/contigs_10K.fa
```

Agora ser√° gerada uma tabela com a informa√ß√£o da cobertura por amostra e subcontig, usando os arquivos `.sorted.bam`.

```
concoct_coverage_table.py 09.CONCOCT/contigs_10K.bed 07.Mapping/*.sorted.bam > 09.CONCOCT/coverage_table.tsv
```

Rode o CONCOCT

```
concoct --composition_file 09.CONCOCT/contigs_10K.fa --coverage_file 09.CONCOCT/coverage_table.tsv --length_threshold 1500 --threads 6 -b 09.CONCOCT/
```
Agora mesclar o agrupamento contig no agrupamento dos contigs originais

```
merge_cutup_clustering.py 08.CONCOCT/clustering_gt1500.csv > 08.CONCOCT/clustering_merged.csv
```
Extrair os bins como arquivos individuais FASTA

```
mkdir 09.CONCOCT/bins

extract_fasta_bins.py 05.Assembly/scaffolds.fasta 09.CONCOCT/clustering_merged.csv --output_path 09.CONCOCT/bins
```

### 6.3. MaxBin2

A terceira ferramenta √© chamada [MaxBin2](https://denbi-metagenomics-workshop.readthedocs.io/en/latest/binning/maxbin.html). 

#### 6.3.1. Instala√ß√£o

O **MaxBin** ser√° instalado no ambiente **Binning**.

```
# Ative o ambiente Binning
conda ativate Binning

# Instale o MaxBin
conda install -c bioconda maxbin2
```

Adicionalmente ser√° necess√°rio instalar tamb√©m a ferramenta [BBMAP](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/) no mesmo ambiente.

```
# Instale o BBMAP
conda install -c bioconda bbmap
```

#### 6.3.2. Uso

Primeiro crie a pasta para sa√≠da do *MaxBin2*

```
mkdir 10.MaxBin2
```

Para obter a informa√ß√£o da cobertura s√£o usados os arquivos `.sam`. Para facilitar, primeiro entre na pasta anteriormente criada 

> üá™üá∏
Para obtener la informaci√≥n de cobertua son usados los archivos `.sam`. Para facilitar, primero entre en la carpeta anteiormente creada.

```
cd 10.MaxBin2/
```

Copie os arquivos gerados no mapeamento (`.sam`) que se encontram na pasta `07.Mapping/` para a pasta atual (`10.MaxBin/`).

```
cp ../07.Mapping/*.sam ./
```

Gere os arquivos de cobertura usando o script `pile.up` da ferramenta **BBMAP**

```
for f in *.sam; do pileup.sh in=$f out=${f%}.txt; done

cd .. ## sair da pasta para ficar na pasta base 10.MaxBin2/
```
Depois gere um arquivo com os nomes e caminhos dos arquivos de cobertura gerados acima
```
ls 10.MaxBin2/*sam.txt > 10.MaxBin2/abundance.list
```
A continua√ß√£o o comando para gerar os bins com *MaxBin*

```
## Crie um diret√≥rio para colocar os bins
mkdir 10.MaxBin2/bins

# Rode o MaxBin
run_MaxBin.pl -contig 05.Assembly/scaffolds.fasta -abund_list 10.MaxBin2/abundance.list -max_iteration 20 -min_contig_length 1500 -thread 10 -out 10.MaxBin2/bins/maxbin
```
*MaxBin2* gera os bins com extens√£o `.fasta`, mas para facilitar as an√°lises downstream e padronizar a mesma extens√£o dos bins gerados por todas as ferramentas, √© melhor converter eles para `.fa`.

Para isto vamos a usar um loop for, para realizar o procedimento com todos bins de uma vez s√≥.

> üá™üá∏
*MaxBin2* genera los bins con extensi√≥n `.fasta`, pero para facilitar los an√°lisis downstram e estandarizar la misma extensi√≥n para los bins generados por todas las herramientas, es mejor convertir todos para `.fa`.

```
cd 10.MaxBin2/bins

for file in *.fasta
do mv "$file" "$(basename "$file" .fasta).fa"
done

ls ## para conferir que agora todos os bins terminam em .fa

cd ../../ # para voltar √† pasta base
```

### 6.4. BinSanity

A quarta ferramenta se chama [BinSanity](https://github.com/edgraham/BinSanity). Esta ferramenta ser√° instalada em um ambiente chamado **Binsanity**

#### 6.4.1. Instala√ß√£o

Primeiro crie o ambiente, e depois instale **Binsanity**

```
# Crie o ambiente
conda create -n Binsanity

# Ative o ambiente
conda activate Binsanity

# Instale Binsanity
conda install -c bioconda binsanity
```

#### 6.4.2 Uso

Como sempre crie uma pasta para a sa√≠da do processamento nesta ferramenta.
```
mkdir 11.BinSanity
```

Gere a informa√ß√£o da cobertura das amostras
```
Binsanity-profile -i 05.Assembly/scaffolds.fasta -s 07.Mapping/ -T 10 -c 11.BinSanity/coverage_profile.txt -o 11.BinSanity/
```
No seguinte comando ser√£o gerados os bins
```
Binsanity-lc -f . -l 05.Assembly/scaffolds.fasta -x 1500 --checkm_threads 1 --Prefix binsanity_ -c 11.BinSanity/coverage_profile.txt.cov.x100.lognorm
```
*BinSanity* criou uma pasta com onde est√£o todos os resultados da corrida `BINSANITY-RESULTS/` e os bins se encontram em  `BINSANITY-RESULTS/binsanity_-KMEAN-BINS/`

Ao igual que com *MaxBin* tem que converter os bins para `.fa`, com a diferen√ßa que o *BinSanity* gera os MAGs com extens√£o `.fna`.

```
cd BINSANITY-RESULTS/binsanity_-KMEAN-BINS/

for file in *.fna
do mv "$file" "$(basename "$file" .fna).fa"
done

ls ## para conferir que agora todos os bins terminam em .fa

cd ../../ # Para voltar √† pasta base
```


## 8. Desreplica√ß√£o com DAS TOOL

o [DasTools](https://github.com/cmks/DAS_Tool) √© uma ferramenta que integra os resultados de diferentes ferramentas de reconstru√ß√£o de genomas apartir de metagenomas, para determinar o conjunto otimizado de MAGs, n√£o redundantes de uma √∫nica montagem.

> üá™üá∏
[DasTools](https://github.com/cmks/DAS_Tool) es una herramienta que integra los resultados de diferentes herramientas de reconstrucci√≥n de genomas a partir de metagenomas, para determinar el conjunto optimizado de MAGs, no redundantes de un √∫nico ensamble

### 8.1 Instala√ß√£o

Crie um ambiente chamado **Dastool** para instalar a ferramenta

```
# Crie o ambiente
conda create -n Dastool

# Ative o ambiente
conda activate Dastool

# Adicione os channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

# Instale Das Tool
conda install -c bioconda das_tool
```

### 8.2 Uso

Crie uma pasta para o output de Das Tool

```
mkdir 12.DasTool
```
O primeiro paso √© gerar uma tabela `.tsv`, com os contigs IDs e os bin IDs.

Nem todas as ferramentas de binning fornecem resultados em formato de tabela `.tsv`, com o contigs IDs e os bin IDs. Para formatar isto o DAS Tool tem um script adicional `scaffolds2bin` que converte um conjunto de bins no formato `.fasta` em um arquivo tabular para ser usado como input do DAS Tool. Salve as tabelas na pasta criada para DAS Tool.

```
cd 08.MetaBat2/01.FirstTrial/ #entrar na pasta da primeira rodada com o MetaBat2

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_firsttrial.tsv #Script pra criar a tabela

cd ../02.SecondTrial/ 

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_secondtrial.tsv

cd ../03.ThirdTrial/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_thirdtrial.tsv

cd ../../09.CONCOCT/bins/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/concoct.tsv

cd ../../10.MaxBin2/bins/

Fasta_to_Scaffolds2Bin.sh -e fa > ../12.DasTool/maxbin.tsv

cd ../BINSANITY-RESULTS/binsanity_-final_bins/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/binsanity.tsv
```

Ap√≥s criadas as tabelas dos bins de cada ferramenta usada, √© rodado o DAS Tool

```
cd ../../12.DasTool/

DAS_Tool -i binsanity.tsv,concoct.tsv,maxbin.tsv,metabat2_firsttrial.tsv,metabat2_secondtrial.tsv,metabat2_thirdtrial.tsv -l binsanity,concoct,maxbin,metabat2ft,metabat2st,metabat3tt -c ../05.Assembly/final.contigs.fa -t 10 -o ./ --score_threshold 0.0 --write_bins 1 --search_engine diamond
```

Ap√≥s terminar a corrida do *DAS Tool*, de uma olhada no arquivo `_DASTool_summary.txt`, o qual √© uma tabela com muitas informa√ß√µes dos bins que passaram o filtro do *DAS Tool* (p.e. "size", "contigs", "N50", "binScore", "SCG_completeness",	"SCG_redundancy")

```{r DAS Tool tabela, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Tabela DASTools"}
knitr::include_graphics("imgs/tabela.png")
```

Adicionalmente, o **DAS Tool** separa os bins que passaram o *threshold* na pasta `12.DasTool/_DASTool_bins/`. 

## 8. Qualidade dos MAGs

#### 8.1. Renomeando os MAGs
           
Crie uma pasta para colocar todos os bins dereplicados pelo DAS Tool
           
```
mkdir 13.MAGS
           
```
           
Use o comando `mv` para colocar os bins dereplicados na pasta `13.MAGS`. Use o comando `ls` para confirmar que tudo deu certo. 

> üá™üá∏
Use el comando `mv` para colocar los bins desreplicados na pasta `13.MAGS`. Use o comando `ls` para confirmar que todo funcion√≥.

```
mv _DASTool_bins/* ../13.MAGS/

ls
```
           
Agora que todos os bins est√£o numa pasta √∫nica, e para facilitar as an√°lises seguintes, renomee os bins usando o script [numerate.sh](https://github.com/khidalgo85/Binning/blob/master/numerate.sh). Este script ir√° renomear, usando um nome base para todos, seguido de n√∫meros consecutivos.
           
> üá™üá∏
Ahora que todos los bins est√°n en la misma carpeta, y para facilitar los an√°lisis siguientes, renombr√© los bins usando el script [numerate.sh](https://github.com/khidalgo85/Binning/blob/master/numerate.sh). Este script renombrar√°, usando un nombre base para todos, seguido de n√∫meros consecutivos.
           
```
./numerate.sh -d 13.MAGS -b 1 -p MAG -s .fa -o numerically -r
```

**SINTAXE**

```
./numerate.sh -d <pasta/com/arquivos/para/renomear> -b <n√∫mero inicial> -p sufijo -s .extens√£o -o numerically
```
           
* `-d`: Pasta com os arquivos que quer renomear
* `-b`: n√∫mero para iniciar a sequ√™ncia
* `-p`: sufijo, palavra inicial
* `-s`: prefijo/extens√£o
* `-o`: ordem (n√∫merica)
           
**Nota:** Caso ao tentar rodar acuse um erro de permiss√£o, digite o seguinte comando `chmod 777 numerate.sh` e tente novamente.
           
Ao final do processo, dentro da pasta `13.MAGS`, ter√° todos os bins, nomeados como MAG1, MAG2, MAG3... MAGn.
           
> üá™üá∏
**Nota:** Si al intentar rodar, sale un error de permisos, d√≠gite el siguiente comando `chmod 777 numerate.sh` e intente nuevamente.
>
Al final del proceso, dentro de la carpeta `13.MAGS`, tendr√° todos los bins nombrados como MAG1, MAG2, MAG3... MAGn.

#### 8.1. CheckM

#### 8.1.1. Instala√ß√£o

A qualidade dos MAGs √© avaliada usando uma ferramenta chamada [CheckM](https://github.com/Ecogenomics/CheckM/wiki). Basicamente a avalia√ß√£o consiste em comparar os MAGs com uma base de dados de genes de c√≥pia √∫nica para assim saber que t√£o completo e contaminado est√° cada um dos genomas recuperados. 

Esta ferramenta pode ser instalada dentro do ambiente **QualityControl**

> üá™üá∏
La calidad de los MAGs es evaluada usando una herramienta llamada [CheckM](https://github.com/Ecogenomics/CheckM/wiki). Basicamente la evaluaci√≥n consiste en comparar los MAGs con una base de datos de genes de copia √∫nica, para as√≠ saber que tan completo y contaminado est√° cada uno de los genomas recuperados.
>
Esta herramienta puede ser instalada dentro del ambiente **QualityControl**

```
# Ative o ambiente QualityControl
conda activate QualityControl

# Instale CheckM
conda install -c bioconda checkm-genome
```

#### 8.1.2. Uso

Ser√£o analisados os genomas desreplicados e renomeados (`13.MAGS`)

Agora crie uma pasta para armazenar a sa√≠da do *CheckM*:

```
mkdir 14.CheckM
```
Para rodar o *CheckM* √© preciso criar um diret√≥rio para os arquivos temporais que ser√£o criados enquanto a corrida.

```
mkdir tmp
```

Lembre **SEMPRE** que **TODA** ferramenta tem um men√∫ de ajuda (`-h` ou `--help`).

Para rodar a an√°lise de qualidade pelo *CheckM* use o seguinte comando:

```
checkm lineage_wf 13.MAGS/ 14.CheckM/ -t 10 -x fa --tmpdir tmp --tab > 14.CheckM/output.txt
```

A continua√ß√£o um gr√°fico mostrando a disperss√£o dos MAGs segundo a qualidade (baseado no [MiMAG](https://www.nature.com/articles/nbt.3893)), sendo poss√≠vel observar quantos MAGs s√£o de baixa (*Low-Quality*: *Completeness < 50 & Contamination > 10*), m√©dia (*Medium-Quality*: *Completeness > 50 & Contamination < 10*) e alta (*High-Quality*: *Completeness > 90 & Contamination < 5*). Para a constru√ß√£o desse gr√°fico, use este  [script](https://github.com/khidalgo85/Binning/blob/master/Checkm.R) de R.

> üá™üá∏
A continuaci√≥n un gr√°fico mostrando la dispersi√≥n de los MAGs segun su calidad (baseado en [MiMAG](https://www.nature.com/articles/nbt.3893)), siendo posible observar cuantos MAGs son de baja (*Low-Quality*: *Completeness < 50 & Contamination > 10*), media (*Medium-Quality*: *Completeness > 50 & Contamination < 10*) y alta (*High-Quality*: *Completeness > 90 & Contamination < 5*). Para la construcci√≥n de este gr√°fico, use este [script](https://github.com/khidalgo85/Binning/blob/master/Checkm.R) de R.

```{r, echo=TRUE, message=FALSE, eval=TRUE}
### Gr√°fico de dispers√£o dos bins baseado na completude e contamina√ß√£o

# Lendo os dados 

tab <- read.delim("output.txt", skip = 33)

library(dplyr)

## Formatando a tabela
tab1 <- as_tibble(tab) %>% 
  na.omit() %>% 
  select(1,12,13) %>% 
  mutate(Quality = case_when(Contamination < 5 & Completeness > 90 ~ "High-Quality",
                             Contamination < 10 & Completeness >= 50 ~ "Medium-Quality",
                             Completeness < 50 ~ "Low-Quality",
                             Contamination > 10 ~ "Low-Quality"))

## Contando os MAGs por qualidade

tab1 %>% 
  group_by(Quality) %>% 
  summarise(
    n=n()
  )


library(ggplot2)

tab1 %>% 
ggplot(aes(x=Completeness, y=Contamination, shape=Quality)) + 
  geom_point(size =1.3) +
  ggtitle("MAGs Dispersion by CheckM") +
  theme(plot.title = element_text(face = "bold", colour = "Dark blue")) +
  theme(axis.title = element_text(face = "bold", colour = "Black")) +
  scale_shape_manual(values = c(17, 8, 1)) +
  ylim(50,0) + xlim(0,100) +
  annotate("text", x = 45, y = 50, label = "High-Quality draft 1 (Comp > 90 & Cont < 5)", size = 2.5, colour= "Dark green") +
  annotate("text", x = 45, y = 45, label = "Medium-Quality draft 7 (Comp > 50 & Cont < 10)", size = 2.5, colour= "Orange") +
  annotate("text", x = 45, y = 40, label = "Low-Quality draft 9 (Comp < 50 & Cont > 10)", size = 2.5, colour= "Red" ) + 
  geom_vline(xintercept=90, linetype="dashed", colour="Dark green") + geom_hline(yintercept=5, linetype="dashed", colour="Dark green") +
  geom_vline(xintercept=50, linetype="dashed", colour="orange") + geom_hline(yintercept=10, linetype="dashed", colour="Orange")


```

Observe que foi poss√≠vel recuperar um genoma de alta qualidade, 7 de m√©dia qualidade e 8 de baixa qualidade. 

## 9. Anota√ß√£o Taxon√¥mica

Existem diversas ferramentas para anota√ß√£o taxon√¥mica, no entanto a mais utilizada em estudos com MAGs √© [GTDB-tk](https://ecogenomics.github.io/GTDBTk/index.html). O qual √© um software criado para asigna√ß√£o taxon√¥mica de genomas de bact√©rias e arqueias baseado no **Genome Database Taxonomy - GTDB**. Dentro desta base de dados existe uma grande quantidade de MAGs obtidos de amostras ambientais e genomas de microrganismos isolados.

> üá™üá∏
Existen diversas herramientas para anotaci√≥n taxon√≥mica, sin embargo la m√°s utilizada en estudios con MAGs es [GTDB-tk](https://ecogenomics.github.io/GTDBTk/index.html). El cual es un software creado para asignaci√≥n taxon√≥mica de genomas de bacterias y arqueas con base en **Genome Database Taxonomy - GTDB**. Dentro de esta base de datos existe una gran cantidad de MAGs obtenidos de muestras ambientales e genomas de micoorganismos aislados.

### 9.1. Instala√ß√£o

A instala√ß√£o de **GTDB-Tk** pode ser feita atrav√©s de conda ao igual que a maioria das ferramentas.

Crie um ambiente para a instala√ß√£o deste programa

> üá™üá∏
La instalacion de **GTDB-Tk** puede ser realizada a trav√©z de conda al igual que la mayoria de herramientas.
>
Crie un ambiente para la instalaci√≥n de este programa

```
# Crie um ambiente e instale GTDB-Tk
conda create -n gtdbtk -c conda-forge -c bioconda gtdbtk
```
Perceba que, na mesma linha de comando voc√™ criou um ambiente chamado gtdbtk e instalou a ferramenta.

Uma vez instalado o programa, √© necess√°rio descarregar a base de dados.

Primeramente ative o ambiente com `conda activate gtdbtk`

A base de dados pode ser descarregada e configurada autom√°ticamente usando o script `download-db.sh`. Esse script ir√° descarregar, descompactar e configurar a DB. 

Esse processo pode ser feito manualmente tamb√©m:

> üá™üá∏
Perciba que, en la misma linea de comando usted cre√≥ un ambiente llamado gtdbtk e instal√≥ la herramienta.
>
Una vez instalado el programa, es necesario descargar la base de datos.
>
La base de datos puede ser descargada y configurada autom√°ticamente usando el script `download-db.sh`. Este script ir√° descargar, descompactar y configurar la DB?
>
Este proceso tambi√©n puede ser hecho manualmente:

```
# Crie um diret√≥rio para fazer download da DB
mkdir dbs
cd dbs/
mkdir gtdb
cd gtdb/

# Download
wget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_data.tar.gz

# Descompactando
tar xvzf gtdbtk_data.tar.gz

# Configurando o PATH
export GTDBTK_DATA_PATH="/caminho/a/dbs/gtdb/release202"
```

Para confirmar que tudo foi corretamente instalado e configurado rode o comando: `gtdbtk check_install`

### 9.2. Uso

Uando R e o arquivo de sa√≠da do CheckM `output.txt` crie uma lista dos MAGs com m√©dia e alta qualidade. A continua√ß√£o encontra o [Script](https://github.com/khidalgo85/Binning/blob/master/filtering_mags.R) do R:

> üá™üá∏
Usando R y el archivo de salida de CheckM `output.txt` cree una lista de los MAGs con media e alta calidad. A continuaci√≥n encontra el [Script](https://github.com/khidalgo85/Binning/blob/master/filtering_mags.R) de R: 

```{r, echo=TRUE, message=FALSE, eval=TRUE}

# Lendo os dados 

tab <- read.delim("output.txt", skip = 33)

library(dplyr)

## Formatando a tabela
tab1 <- as_tibble(tab) %>% 
  na.omit() %>% 
  select(1,12,13) %>% 
  mutate(Quality = case_when(Contamination < 5 & Completeness > 90 ~ "High-Quality",
                             Contamination < 10 & Completeness >= 50 ~ "Medium-Quality",
                             Completeness < 50 ~ "Low-Quality",
                             Contamination > 10 ~ "Low-Quality"))

## Contando os MAGs por qualidade

tab1 %>% 
  group_by(Quality) %>% 
  summarise(
    n=n()
  )

### Criando tabela s√≥ com os MAGs de alta e m√©dia qualidade
mags.filtered <- tab1 %>% 
  group_by(Quality) %>% 
  filter(Quality == "High-Quality" | Quality == "Medium-Quality") %>% 
  ungroup() %>% 
  select(Bin.Id) %>% 
  as.data.frame()

fa <- c(rep(".fa", nrow(mags.filtered))) ## add .fa a cada nome do MAG


mags <- mags.filtered[,1] %>% 
  paste0(., fa) %>% 
  write(., file="filt.mags.txt") # salva o arquivo com a lista dos Mags que passaram o filtro


mags.filtered
```

Fa√ßa upload do arquivo criado com o script do R `filt.mags.txt` no servidor dentro da pasta base. Este arquivo ser√° usado para separar os MAGs de alta e m√©dia qualidade dos de baixa que n√£o ser√£o usados nas seguintes an√°lises. 

> üá™üá∏
Haga el upload del archivo creado com el script de R `filt.mags.txt` al servidor dentro de la pasta base. Este archivo ser√° usado para separa los MAGs de alta y media calidad de los de baja calidad que no ser√°n usados en los siguentes an√°lisis.

```
# Entre na pasta dos MAGs
cd 13.MAGS

# Crie um diret√≥rio para colocar os MAGs com alta e m√©dia qualidade
mkdir HQ_MQ_MAGs

# Crie um diret√≥rio para colocar os MAGs com baixa qualidade
mkdir LQ_MAGs
```

Uma vez criados os diret√≥rios para separar os MAGs pela qualidade, ser√° usado o arquivo com a lista de MAGs que passaram o filtro:

> üá™üá∏
Una vez creados los directorios para separar los MAGs por la calidad, ser√° usado un archivo con la lista de MAGs que pasaron el filtro:

```
cat ../filt.mags.txt | xargs mv -t HQ_MQ_MAGs/

ls HQ_MQ_MAGs
```

Com o comando anterior, os MAGs que estiverem na lista do arquivo `filt.mags.txt` ser√£o trocados de pasta para o diret√≥rio `HQ_MQ_MAGs/`. Se preferir, para organizar melhor, passe o restante dos MAGs dentro da pasta `13.MAGS/` (MAGs com baixa qualidade) para o diret√≥rio `LQ_MAGs/`.

> üá™üá∏
Con el comando anterior, los MAGs que est√©n en la lista del archivo `filt.mags.txt` ser√°n cambiados de carpeta para el directorio `HQ_MQ_MAGs/`. Si prifiere, para organizar mejor, pase el restante de MAGs dentro de la carpeta `13.MAGS` (MAGs con baja calidad) para el directorio `LQ_MAGs/`.

```
mv *.fa LQ_MAGs/
```

Pronto, agora pode proceder √† anota√ß√£o taxon√¥mica dos MAGs de alta e m√©dia qualidade usando o programa **GTDB-tk**.

```
cd ..

mkdir 15.TaxonomicAnnotation

gtdbtk classify_wf --genome_dir 13.MAGS/HQ_MQ_MAGs/ --out_dir 15.TaxonomicAnnotation/ -x fa --min_perc_aa 10 --cpus 15 --scratch_dir slower
```

**SINTAXE**
```
gtdbtk classify_wf --genome_dir diretorio/com/os/Mags/ --out_dir output/ -x <fa fasta fna> --cpus <n√∫mero de n√∫cleos>
```

* `classify_wf`: este script consiste em tr√™s fases (que tamb√©m podem ser feitas por separado); `identify`, `align` e `classify`. `identify` ger√° uma predi√ß√£o de genes usando **Prodigal** e usa **HMMER** para identificar 120 genes marcadores de bact√©ria e archaea usados para infer√™ncia flogen√©tica. S√£o feitos alinhamentos m√∫ltiplos no alinhamento dos marcadores nos modelos HMM. `align` concatena os marcadores de genes alinhados e filtra o alinhamento multiplo a aproximadamente 5,000 amino √°cidos. `classify`, usa **pplacer** usando *maximun-likelihood* para colocar cada genoma numa √°rvore de refer√™ncia de GTDB-Tk, o qual classifica cada genoma baseado no seu lugar na √°rvore de refer√™ncia, a sua diverg√™ncia evolucionaria relativa, e/ou o ANI (*average nucleotide identity*) contra genomas de refer√™ncia. 

> üá™üá∏
* `classify_wf`: este script consiste en tres fases (que tambi√©n pueden ser realizadas por separado); `identify`, `align` e `classify`. `identify` genera una predicci√≥n de genes usando **Prodigal** y usa **HMMER** para identificar 120 genes marcadores de bacteria e arquea usados para inferencia flogen√©tica. Son realizados alineamintos m√∫ltiples en el alineamiento de los marcadores en los modelos HMM. `align` concatena los marcadores de genes alineados y filtra el alineamiento multiple a aproximadamente 5,000 amino √°cidos. `classify`, usa **pplacer** usando *maximun-likelihood* para colocar cada genoma en un √°rbol de referencia de GTDB-Tk, oel cual clasifica cada genoma con base al lugar en el √°rbol de referencia, a su divergencia evolucionaria relativa, y/o el ANI (*average nucleotide identity*) contra genomas de referencia. 


* `--genome_dir`: Diret√≥rio com os genomas

* `out_dir`: Diret√≥rio para o output. Nele ser√£o criadas autom√°ticamente pastas para cada uma das fases do processo.

* `-x`: formato (p.e. `.fa`, `.fasta`, `.fna`)

* `--cpus`: n√∫mero de n√∫cleos ou threads.


### 9.3. Output

Ap√≥s a corrida de **GTDB-Tk** ser√£o gerados uma s√©rie de arquivos e pastas com resultados de cada fase do processo. Esta ferramenta separa os resultados dos MAGs que foram anotados como bact√©rias e como Arqueias. Isto devido a que usa dois bases de dados separadas para cada dominio. Ass√≠m, os arquivos com os resultados finais s√£o `gtdbtk_bac120.summary.tsv` e `gtdbtk_ar122.summary.tsv`. Estes arquivos s√£o tabelas com as seguintes colunas:

> üá™üá∏
Depois del proceso de **GTDB-Tk** ser√°n generados una serie de archivos y directorios con resultados de cada fase del proceso. Esta herramienta separa los resultados de los MAGs que fueron anotados como bacterias y arqueas. Este es debido a que usa dos bases de datos separadas para cada dominio. As√≠, los archivos con los resultados finales son: `gtdbtk_bac120.summary.tsv` y `gtdbtk_ar122.summary.tsv`. Estos archivos son tablas con las siguientes columnas: 

* user_genome: Nome do MAG
* Classification: Taxonomia inferida por GTDB-Tk. 
* fastani_reference: Indica o n√∫mero de acesso do genoma de refer√™ncia ao qual o MAG foi assignado baseado no ANI (*Average Nucleotide Identity*)
* fastani_reference_radius: Informa√ß√£o n√∫merica do ANI
* fastani_taxonomy: Indica a taxonomia do GTDB para o genoma de refer√™ncia
* fastani_ani: Indica o valor de ANI entre o MAG e o genoma de refer√™ncia
* fastani_af: infica o valor de AF entre o MAG e o genoma de refer√™ncia
* closest_placement_reference: Indica o n√∫mero de acesso ao genoma de refer√™ncia quando o MAG e colocado em um branch terminal
* closest_placemente_taxonomy: Indica a taxonomia do GTDB para o genoma de refer√™ncia de cima
* closest_placement_ani: Indica o valor de ANI entre o MAG e o genoma de refer√™ncia de cima.
* closest_placement_af: Indica o valor de AF entre o MAG e o genoma de refer√™ncia de cima
* pplacer_taxonomy: Indica a taxonomia do pplacer para o MAG
* classification_method: Indica o m√©todo usado para a classifica√ß√£o do MAG.
* note: provee informa√ß√£o adicional da clasifica√ß√£o do MAG. Este campo √© prenchido quando a determina√ß√£o da esp√©cie √© feita e indica se a coloca√ß√£o de MAG dentro de uma √°rvore de refer√™ncia e o parente mais pr√≥ximo baseado no ANI/AF s√£o iguais (congruent) ou diferentes (incongruent).
* other_relate_references: lista de 100 genomas de refer√™ncia pr√≥ximos baseadso no ANI.
* mas_percent: Indica a porcentagem do MSA 

**TABELA**

Usando R, contrua uma tabela √∫nica com as informa√ß√µes das taxonomias e da qualidade dos MAGs:

```{r, message=FALSE}
## Construindo tabela com taxonomia e qualidade dos mags

## install.packages('dplyr')
library(dplyr)

## install.packages('tidyr')
library(tidyr)


## Lendo a tabela sa√≠da do CheckM com a qualidade dos MAGs
quality <- read.delim("output.txt", skip = 33) %>% 
  select(Bin.Id, Completeness, Contamination) %>% 
  na.omit() %>% 
  as_tibble() %>% 
  rename(Genome = Bin.Id)

## Lendo a tabela de anota√ß√£o taxon√¥mica de bact√©rias
bact.taxa <- read.delim("gtdbtk.bac120.summary.tsv",
                        sep = '\t') %>% 
  select(user_genome, classification, classification_method, note) %>% 
  rename(Bin.Id = user_genome) %>% 
  as_tibble() %>% 
  separate(col = classification, into = c("Kingdom", "Phylum", "Class", "Order",
                                          "Family", "Genus", "Species"), sep = ';')



## Lendo a tabela de anota√ß√£o taxon√¥mica de Arqueas
arc.taxa <- read.delim("gtdbtk.ar122.summary.tsv",
                       sep = '\t') %>% 
  select(user_genome, classification, classification_method, note) %>% 
  rename(Bin.Id = user_genome) %>% 
  as_tibble() %>% 
  separate(col = classification, into = c("Kingdom", "Phylum", "Class", "Order",
                                          "Family", "Genus", "Species"), sep = ';')


## Unindo as tabelas de taxonomia
taxa <- rbind(bact.taxa,  arc.taxa) %>%
  rename(Genome = Bin.Id) 

### Limpando a taxonomia
temp <- select(taxa, 2:8)

# Eliminando caracteres indesejados na taxonomia 
temp$Kingdom<-gsub("d__","",as.character(temp$Kingdom))
temp$Phylum<-gsub("p__","",as.character(temp$Phylum))
temp$Class<-gsub("c__","",as.character(temp$Class))
temp$Order<-gsub("o__","",as.character(temp$Order))
temp$Family<-gsub("f__","",as.character(temp$Family))
temp$Genus<-gsub("g__","",as.character(temp$Genus))
temp$Species<-gsub("s__","",as.character(temp$Species))


### Completando os campos sem taxonomia com o √∫ltimo n√≠vel assignado
temp[is.na(temp)] <- ""

for (i in 1:nrow(temp)){
  if (temp[i,2] == ""){
    Kingdom <- paste("Kingdom_", temp[i,1], sep = "")
    temp[i, 2:7] <- Kingdom
  } else if (temp[i,3] == ""){
    phylum <- paste("Phylum_", temp[i,2], sep = "")
    temp[i, 3:7] <- phylum
  } else if (temp[i,4] == ""){
    Class <- paste("Class_", temp[i,3], sep = "")
    temp[i, 4:7] <- Class
  } else if (temp[i,5] == ""){
    Order <- paste("Order_", temp[i,4], sep = "")
    temp[i, 5:7] <- Order
  } else if (temp[i,6] == ""){
    Family <- paste("Family_", temp[i,5], sep = "")
    temp[i, 6:7] <- Family
  } else if (temp[i,7] == ""){
    temp$Species[i] <- paste("Genus",temp$Genus[i], sep = "_")
  }
}

taxa$Kingdom <- temp$Kingdom
taxa$Phylum <- temp$Phylum
taxa$Class <- temp$Class
taxa$Order <- temp$Order
taxa$Family <- temp$Family
taxa$Genus <- temp$Genus
taxa$Species <- temp$Species



## Unindo a taxonomia com a qualidade
taxa <- merge(quality, taxa, by = "Genome") 



# Classificando os MAGs pela qualidade
taxa2 <- taxa %>% 
  mutate(Quality = if_else(Completeness > 90 & Contamination < 5, "HighQuality",
                           "MediumQuality"))

```

```{r, echo=FALSE, message=FALSE}

library(dplyr)
library(kableExtra)

df<- taxa2
color.me <- which(df$Quality == "HighQuality")

color.me1 <- which(df$Quality == "MediumQuality")

df %>% 
  kable(escape = F, full_width = FALSE) %>%
  kable_styling() %>%
  row_spec(color.me, bold = T, color = "white", background = "green") %>% 
  kable_styling() %>%
  row_spec(color.me1, bold = T, color = "white", background = "orange") %>% 
  row_spec(0, background = "rgb(172, 178, 152)", color = "black", font_size = 15)
```

## 10. Abund√¢ncia relativa dos MAGs nas amostras

Usando o programa [**CoverM**](https://github.com/wwood/CoverM) √© poss√≠vel calcular a abund√¢ncia relativa de cada MAG em cada uma das amostras. Para isto √© necess√°rio usar os arquivos `.sorted.bam` para mapear os genomas dentro das reads das amostras.

> üá™üá∏
Usando el programa [**CoverM**](https://github.com/wwood/CoverM) es posible calcular la abundancia relativa de cada MAG en cada una de las muestras. Para esto es necesario usar los archivos `.sorted.bam` para mapear los genomas dentro de los reads de las muestras 

### 10.1. Instala√ß√£o

Crie um novo ambiente para esta ferramenta, chamado **coverm**

```
# Crie o ambiente
conda create -n coverm

# Ative o ambiente
conda activate coverm

# Instale Coverm
conda install -c bioconda coverm
```

### 10.2 Uso

**CorverM** usar√° a informa√ß√£o do mapeamento, para calcular a porcentagem relativa de cada MAG em cada uma das amostras que os originaram. 

```
mkdir 16.Coverage

coverm genome --bam-files 06.Mapping/Sample1.sorted.bam 06.Mapping/Sample2.sorted.bam 06.Mapping/Sample3.sorted.bam 06.Mapping/Sample4.sorted.bam 06.Mapping/Sample5.sorted.bam 06.Mapping/Sample6.sorted.bam -d 13.MAGs/HQ_MQ_MAGs -x fa --min-read-percent-identity 0.95 --methods relative_abundance --output-file 16.Coverage/output_coverm.tsv
```
**SINTAXE**

```
coverm mode <mapping_input> -d genomes_directory/ -x extension <.fa .fna .fasta> --min-read-percent-identity x.xx --methods method --output-file output.tsv -t 6
```

* `mode`: `genome` ou `contig.` Tipo de dados nos que o programa vai calcular a cobertura

* `mapping_input`: 
  * `-1` (arquivos forward fasta/q para mapeamento), `-2` (arquivos reverse fasta/q para mapeamento)
  * `-c` um o mais pares de forward e reverse fasta/q para mapeamento em ordem
  * `--single` fasta/q no pareados (single-end)
  * `--bam-files` archivos bam ordenados (sorted)
* `-d` diret√≥rio com os genomas
* `-x` extens√£o dos arquivos dos genomas
* `--min-read-percent-identity` Exclui sequ√™ncias com porcetagem de identidade menor ao inserido.
* `--methods` M√©todo para calcular a cobertura (i.e `relative_abundance`, `mean`, `trimmed_mean`, `coverage_histogram`, `covered_bases`, `varieance`, etc)
* `--output-file` tabela com resultados. 
* `-t` n√∫mero de threads

**Nota:** Este programa tem **MUITOS** par√¢metros para customizar, por favor para mais informa√ß√£o visite a documenta√ß√£o pro modo [`genome`](wwood.github.io/CoverM/coverm-genome.html) ou pro modo [`contig`](wwood.github.io/CoverM/coverm-contig.html).

Agora adicione a nova informa√ß√£o de abund√¢ncia relativa de cada MAG na tabela com a qualidade e a taxonomia e gr√°fique

```{r, message=FALSE}
library(tidyr)
library(ggplot2)

## Lendo a tabela de cobertura

cov <- read.delim("output_coverm.tsv") %>% 
  rename(Sample1 = Sample1_.sorted.Relative.Abundance....,
         Sample2 = Sample2_.sorted.Relative.Abundance....,
         Sample3 = Sample3_.sorted.Relative.Abundance....,
         Sample4 = Sample4_.sorted.Relative.Abundance....,
         Sample5 = Sample5_.sorted.Relative.Abundance....,
         Sample6 = Sample6_.sorted.Relative.Abundance....
  ) %>% 
  filter(Genome != "unmapped") %>% 
  mutate_if(is.numeric, round, digits = 2)


## Unindo com a tabela de taxonomia e qualidade

final.table <- merge(taxa2, cov, by = "Genome")

## Transpondo

df <- final.table %>%
  gather(Sample, RelativeAbundance, Sample1:Sample6) %>% 
  as_tibble()


mypalette <- c("#a65538","#7282e2","#6fbe48","#ad5ed3","#bbb237",
               "#6256ba","#e29a39","#6086c3")


### Phylum
p <- df %>% 
  ggplot(aes(y=Sample, x=RelativeAbundance, fill=Family)) + 
  geom_bar(stat='identity') + scale_fill_manual(values=c(mypalette)) +
  scale_y_discrete(labels=c("Sample1", "Sample2", "Sample3", "Sample4", 
                            "Sample5", "Sample6")) +
  theme(axis.text.x = element_text(size = 14)) +
  theme(axis.text.y = element_text(size = 14)) +
theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(size = 14)) +
  theme(axis.text.y = element_text(size = 14)) +
  theme(legend.text = element_text(size = 12))

p
```


```{r, echo=FALSE, message=FALSE}

library(dplyr)
library(kableExtra)

df<- final.table
color.me <- which(df$Quality == "HighQuality")

color.me1 <- which(df$Quality == "MediumQuality")

df %>% 
  kable(escape = F, full_width = FALSE) %>%
  kable_styling() %>%
  row_spec(color.me, bold = T, color = "white", background = "green") %>% 
  kable_styling() %>%
  row_spec(color.me1, bold = T, color = "white", background = "orange") %>% 
  row_spec(0, background = "rgb(172, 178, 152)", color = "black", font_size = 15)
```

As colunas com *NaN* significa que nenhuma read dessas amostras foram mapeadas. Isso pode ser porque as amostras do dataset de exemplo s√£o pequenas subsamples. Nenhum MAG foi gerado a partir dessas duas amostras (Sample4 e Sample6).

> üá™üá∏
Las columnas con *NaN* significa que ning√∫n read de esas muestras fueron mapeadas. Esto puede ser porque las muestras del dataset de ejemplo son peque√±os subsampels. Ning√∫n MAG fue generado a partir de esas dos muestras (Sample4 y Sample6).

## 11. Anota√ß√£o Funcional

Al√©m da anota√ß√£o taxon√¥mica pode ser feita uma anota√ß√£o funcional para conhecer o potencial metab√≥lico de cada um dos MAGs obtidos. Esta fase est√° divida em dois grandes processos: i) predi√ß√£o dos genes, ii) alinhamento dos genes preditos contra as diferentes bases de dados.

> üá™üá∏
Adem√°s de la anotaci√≥n taxon√≥mica puede ser hecha una anotaci√≥n funcional para conocer el potencial metab√≥lico de cada uno de los MAGs obtenidos. Esta fase est√° dividida en dos grandes procesos: i) predicci√≥n de genes, ii) alineamiento de los genes predichos contra las diferentes bases de datos.

### 11.1. Predi√ß√£o de genes

O objetivo desta etapa √© procurar os mascos abertos de leitura ou ORF (*Open Reading Frames*) dentro dos MAGs. Ou seja, predizer onde inicam e terminam os genes. Basicamente o programa procura por codons de inicio, principalmente **ATG**, por√©m tamb√©m s√£o codons de inicia√ß√£o **GTG** e **TTG**. Depois, o programa procura os codons de parada, como **TAA, TAG** e **TGA**.

> üá™üá∏ El objetivo de esta etapa es buscar los marcos abiertos de lectura o ORF (en ingl√©s) dentro de los MAGs. O sea, predecir donde incian y terminan los genes. Basicamente el programa busca por c√≥dones de inicio, principalmente **ATG**, sin embargo tambi√©n son c√≥dones de inico **GTG** e **TTG**. Despu√©s, busca los c√≥dones de parada, como **TAA**, **TAG** y **TGA**.

O programa a usar para a predi√ß√£o das ORFs em procarioros √© [Prodigal (*Prokaryotic Dynamic Programming Gene Fiding Algorithm*)](https://github.com/hyattpd/Prodigal).

#### 11.1.1 Instala√ß√£o

Crie um novo ambiente para instala√ß√£o das ferramentas relacionadas com √† anota√ß√£o de genes, chamado **Annotation**

```
# Crie o ambiente
conda create -n Annotation

# Ative o ambiente
conda activate Annotation

# Instale Prodigal
conda install -c bioconda prodigal
```

#### 11.1.2. Uso

Crie uma pasta chamada `17.GenePrediction` para colocar a sa√≠da do **Prodigal**.

`mkdir 17.GenePrediction`

A continua√ß√£o encontrara o comando **individual** (Um MAG por vez)

```
prodigal -i 13.MAGS/HQ_MQ_MAGs/MAG1.fa -f gff -o 17.GenePrediction/MAG1.gff -a 17.GenePrediction/MAG1.faa -d 17.GenePrediction/MAG1.fa -p single
```

Se quiser pode rodar a an√°lises para v√°rios MAGs ao mesmo tempo, usando o seguinte loop:

```
for i in 13.MAGS/HQ_MQ_MAGs/*.fa
do
BASE=$(basename $i .fa)
prodigal -i $i -f gff -o 17.GenePrediction/${BASE}.gff -a 17.GenePrediction/${BASE}.faa -d 17.GenePrediction/${BASE}.fa -p single
done
```

**SINTAXE**

```
prodigal -i genome.fasta -f <gbk, gff, sqn, sco> -o coordfile -a proteins.faa -d nucleotides.fa
```
* `-i`: caminho para o genoma em formato `.fasta`, `.fa` ou `.fna`
* `-f`: formato de sa√≠da para o arquivo de coordenadas, default `.gbk`(*Genbank-like format*), `.gff` (`Gene Feature Format`), `.sqn` (*Sequin feature format*), ou `.sco` (*Simple Coordinate Output*)
* `-o`: arquivo output com as coordenadas das ORFs
* `-a`: sequ√™ncias das ORFs em prote√≠na
* `-d`: sequ√™ncias das ORFs em nucleot√≠deos

**Formato `.gff` (Gene Feature Format)**

üáßüá∑ Este formato guarda as informa√ß√µes dos genes preditos pelo Prodigal. Explore-o (`less GenesCoordenates.gff`).

Cada sequ√™ncia comen√ßa com um *header* com as informa√ß√µes da sequ√™ncia analizada, seguido de uma tabela separada por tabula√ß√µes com informa√ß√µes dos genes encontrados em dita sequ√™ncia.

O *header* cont√©m os seguentes campos:

> üá™üá∏ Este formato guarda las informaciones de los genes predichos por Prodigal. Explorelo (`less GenesCoordenates.gff`).
> 
> Cada secuencia comienza con un *header* con las informaciones de la secuencia analizada, seguido de una tabla separada por tabulaciones con informaciones de los genes encontrados en dicha secuencia.
> 
> El *header* contiene los siguientes campos:

* **seqnum**: O n√∫mero da sequ√™ncia, come√ßando pelo n√∫mero 1.
* **seqlen**: tamanho em bases da sequ√™ncia
* **seqhdr**: t√≠tulo completo da sequ√™ncia extra√≠do do arquivo `.fasta`.
* **version**: vers√£o do Prodigal usado
* **run_type**: modo de corrida, p.e. m*metagenomic*
* **model**: informa√ß√£o sob o arquivo de treinamento usado para a predi√ß√£o.
* **gc_cont**: % de GC na sequ√™ncia
* **transl_table**: Tabela do c√≥digo gen√©tico usada para analizar a sequ√™ncia. Para bact√©rias e archaeas √© usada a [tabela 11](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi#SG11).
* **uses_sd**: 1 se o Prodigal usa o *[RBS](https://parts.igem.org/Ribosome_Binding_Sites) finder*, ou 0 se usa outros *motifs*. 

Despu√©s do *header* se encuentra una tabla con las informaciones de los genes encontrados:

* **seqname**: nome da sequ√™ncia, neste caso nome do scaffold/contig.
* **source**: nome do programa que gerou a predi√ß√£o
* **feature**: tipo de *feature*, p.e. CDS (*Coding DNA Sequence*)
* **start**: primeira posi√ß√£o da *feature*
* **end**: √∫tlima posi√ß√£o da *feature*
* **score**: Valor numerico que geralmente indica a confian√ßa do programa na predi√ß√£o da ORF.
* **strand**: fita do DNA que foi encontrado a *feature*. A fita *forward* √© definida como '+', e a *reverse* como '-'.
* **frame**: 0 indica que a primeira base da *feature* √© a primeira base do c√≥don de inicio, 1, que a segunda base da *feature* √© a primeira base do c√≥don de inicio.
* **atribute**: informaci√≥n adicional sobre la *feature*, parada por ponto e v√¨rgula ";".  

  * **ID**: identificador √∫nico de cada gene, consistindo em um n√∫mero ordinal ID da sequ√™ncia e um n√∫mero ordinal ID do n√∫mero do gene separados por "_". Por exemplo "1_688" siginifa que √© o gene n√∫mero 688 da sequ√™ncia 1.
  * **partial**: indica se o gene est√° completo ou n√£o. "0" indica que no gene foi encontrado o c√≥don de inicio ou de parada, "01" indica que no gene s√≥ foi encontrado o c√≥ndon de inicio, "11" indica que n√£o foram encontrados nenhum dos dois c√≥dons e "00" indica que foram encontrados ambos c√≥dons.
  * **start_type**: sequ√™ncia do c√≥don de inicio.
  * **stop_type**: sequ√™ncias do c√≥don de parada
  * **rbs_motif**: *RBS motif* encontrado pelo Prodigal
  * **rbs_spacer**: n√∫mero de bases entre o c√≥don de inicio e o *motif* observado.
  * **gc_cont**: Conet√∫do de GC no gene
  * **conf**: nota de confian√ßa pra o gene, representa a probabilidade que esse gene seja real. 
  * **score**: *score* total pro gene
  * **cscore**: fra√ß√£o hexamero do *score*, o quanto este gene se parece com uma prote√≠na verdadeira.
  * **sscore**: *score* para o sitio de inicio da tradu√ß√£o do gene. √© a soma dos tr√™s seguintes *scores*.
  * **rscore**: *score* pro *RBS motif*
  * **uscores**: *score* pra sequ√™ncia em torno do c√≥don de in√≠cio.
  * **tscore**: *score* para o tipo de c√≥don de inicio
  * **mscore**: *score* pros sinais restantes (tipo de c√≥don de parada e informa√ß√µes da fita principal / reversa).
  

Uma vez terminado o processo, pode explorar os diferentes arquivos de sa√≠da para conhecer a fondo a estrutura de cada um deles e as informa√ß√µes que cada um tem.

### 11.2. Anota√ß√£o Funcional

A anota√ß√£o dos genes √© feita alinhando as ORFs preditas contra bases de dados. No caso da anota√ß√£o funcional, ser√° usado o alinhador [**Diamond**](https://github.com/bbuchfink/diamond) e as bases de dados ser√£o [**EggNOG**](http://eggnog5.embl.de/#/app/home) e [**KEGG**](https://www.kegg.jp/kegg/).

> üá™üá∏La anotaci√≥n de los genes es realizada alineando las ORFs predichas contra bases de dados. En el caso de la anotaci√≥n funcional ser√° usado el programa para alineamiento [**Diamond**](https://github.com/bbuchfink/diamond) y las bases de datos [**EggNOG**](http://eggnog5.embl.de/#/app/home) y [**KEGG**](https://www.kegg.jp/kegg/).

#### 11.2.1 Diamond

##### 11.2.1.1 Obten√ß√£o das Bases de Dados

Para a obten√ß√£o das bases de dados, pode ir nos sites e descarregar diretamente. No entanto, tenha em conta que a base de dados **KEGG** √© paga. Se voc√™ descarregar direto da fonte, dever√° formatar as DBs para o seu uso com Diamond (anota√ß√£o funcional). Isto √© feito com o comando `makedb --in reference.fasta -d reference`.

Para facilitar, no seguinte link, voc√™ encontrar√° as bases de dados **KEGG**, **EggNOG**, previamente formatadas para o uso em Diamond.   

Use o programa `gdown` para descarregar as dbs que se encontram em um GoogleDrive. Se n√£o tiver o `gdown` instalado, siga o seguintes passos: 

> üá™üá∏ Para la obtenci√≥n de las bases de datos, puede ir directamente en las p√°ginas web de cada una. Sin embargo, tenga en cuenta que la base de datos **KEGG** es paga. Si ud decide descargar directamente de la fuente, deber√° hacer una formataci√≥n de las DBs para el uso con Diamond (anotaci√≥n funcional). Este processo es realizado usando el comando `makedb --in reference.fasta -d reference`. 
>
> Para facilitar, en el siguiente link, encontrar√° las bases de datos**KEGG**, **EggNOG**, previamente formatadas para su uso en Diamond e **Kraken2**. 


* [**Dbs**](https://drive.google.com/drive/folders/1GLP6vA4Gs0cce-nnBXCmZSgmONWybOSF?usp=sharing)


```
## Se n√£o tiver instalado pip
sudo apt update
sudo apt install python3-pip
pip3 --verision

## Instale gdown
pip install gdown
```

üáßüá∑ Crie uma pasta, chamada `dbs/`, e use o programa `gdown` para descarregar as dbs. 

```
# Crie o diret√≥rio
mkdir dbs/

# Descarregue as DBs

## KEGG
gdown --id 1ZxjJdwh1izP32X5CH-B8SN0DK2WAAAvr

##EggNOG
gdown --id 1x2Kp4PTX8GFFhkJm6EVDQLfi-xRSQ735
```

Ser√£o descarregados os seguintes arquivos:

* `eggnog.dmnd`: Base de dados EggNOG formatada para Diammond
* `keggdb.dmnd`: Base de dados KEGG formatada para Diammond


#### 11.2.1.2 Instala√ß√£o Diammond

O [**Diamond**](https://github.com/bbuchfink/diamond) ser√° usado para a anota√ß√£o funcional. Instale atrav√©s do conda, no ambiente `Annotation`

```
# Active o ambiente
conda activate Annotation

# Instala√ßao
conda install -c bioconda diamond=2.0.9
```
### 11.2.2 Uso

üáßüá∑ Uma vez instaladas todas as ferramentas e descarregadas as bases de dados, pode proceder √† anota√ß√£o. Neste caso ser√° feita primeiro √† funcional, usando Diammond e as bases de dados **KEGG** e **EggNOG**.
A continua√ß√£o se encontra o comando ndividual (*uma montagem e uma base de dados por vez*)

> üá™üá∏ Una vez instaladas todas las herramientas y descargadas las bases de datos, puede proceder a la anotaci√≥n. En este caso ser√° hecha primero la anotaci√≥n funcional, usando Diammond e las bases de datos **KEGG** e **EggNOG**

```
## Crie uma pasta pra sa√≠da
mkdir 18.FunctionalAnnotation

## Diammond
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d dbs/keggdb.dmnd --query 17.GenePrediction/GenesNucl.fa -o 18.FunctionalAnnotation/GenesNucl_kegg.txt --tmpdir /dev/shm
```

**SINTAXE**

```
diamond blastx --more-sensitive --threads -k -f --id --query-cover -d dbs/db.dmnd --query orfs_nucleotides.fa -o annotation.txt --tmpdir /dev/shm
```

* `blastx`: Alinha sequ√™ncias de DNA contra uma base de dados de prote√≠nas
* `--more-sensitive`: este modo permite hits com >40% de identidade. Existem outros modos `--fast --min-sensitive --very-sensitive --ultra-sensitive`. Clique [aqui](https://github.com/bbuchfink/diamond/wiki/3.-Command-line-options) para mais detalhes
* `--threads`: n√∫mero de n√∫cleos
* `-k/--max-target-seqs`: N√∫mero m√°ximo de sequ√™ncias *target* por *query* para reportar alinheamentos.
* `-f/--outfmt`: Formato de sa√≠da. S√£o aceptos os seguintes valores:
  * `0` Formato BLAST *pairwise*
  * `5` fomato BLAST XML
  * `6` Formato do BLAST tabular (default), pode customizar as colunas com uma lista separada por espa√ßos, das seguintes op√ß√µes:
    * `qseqid` id da sequ√™ncia *query*
    * `qlen` tamanho da sequ√™ncia *query* 
    * `sseqid` id da sequ√™ncia da base de dados
    * `sallseqid` todas os id das sequ√™ncias das bases de dados
    * `slen` tamanho da sequ√™ncia da base de dados
    * `qstart` inicio do alinhamento no *query*
    * `qend` fim do alinhamento no *query*
    * `sstart` inicio do alinhamento na sequ√™ncia da base de dados
    * `send` fim do alinhamento na sequ√™ncia da base de dados
    * `evalue` 
    * `bitscore` 
    * `score` 
    * `length` tamanho do alinhamento
    * `pident` porcentagem de matches identicos
    
Com o comando anterior foi feita a anota√ß√£o do co-assembly de todas as amostras `scaffolds.fasta` com a base de dados `kegg.dmnd` e os dados foram guardados no arquivo `kegg_annotation.txt`.

> üá™üá∏ Con el comando anterior fue realizada la anotaci√≥n del co-assembly de todas las muestras `scaffolds.fasta` con la base de datos `kegg.dmnd` y los datos fueron guardadas en el archivo `GenesNucl_kegg.txt`. 


Se tiver mais de uma montagem e quiser rodar todas e as duas bases de dados ao mesmo tempo, pode usar o seguinte loop `for`:

> üá™üá∏ Si tiene m√°s de un ensamble y quiere correr todos e las dos bases de datos al mismo tiempo, puede usar el siguiente loop `for`:

```
for i in 17.GenePrediction/*.fa
do
BASE=$(basename $i .fa)
  for j in dbs/*.dmnd
  do
  db=$(basename $j .dmnd)
diamond blastx --more-sensitive --threads 6 -k 1 -f 6 qseqid qlen sseqid sallseqid slen qstart qend sstart send evalue bitscore score length pident qcovhsp --id 60 --query-cover 60 -d $j --query $i -o 18.FunctionalAnnotation/${BASE}_${db}.txt --tmpdir /dev/shm
done
done
```

Com o comando anterior, √© feita a anota√ß√£o em todas as ORFs preditas na pasta `17.GenePrediction/` com todas as bases de dados para diammond dentro da pasta `dbs/`. Veja que no loop foram declaradas duas variav√©is, `i` que corresponde a cada um dos arquivos das ORFs (nucleot√≠deos) preditas com Prodigal e a vari√°vel `j` que corresponde a cada um dos arquivos terminados em `.dmnd` dentro da pasta `dbs/`, ou seja as bases de dados `kegg.dmnd` e `eggnog.dmnd`. Os arquivos de sa√≠da s√£o duas tabelas por cada montagem, uma da anota√ß√£o com *eggnog* e outra com *kegg*. 

> üá™üá∏ Con el comado anterior, es realizada la anotaci√≥n de todas las ORF predichas en el directorio `17.GenePrediction/` con todas las bases de datos para Diammond dentro de la carpeta `dbs/`. Vea que en el loop fueron declaradas dos variables, `i` que corresponde a cada uno de los archivos de las ORFs (nucle√≥tidos) predichos con Prodigal e la variable `j` que corresponde a cada uno de los archivos terminados en `.dmnd` dentro de la carpeta `dbs/`, o sea las bases de datos `kegg.dmnd` y `eggnog.dmnd`. Los archivos de salida son dos tablas por cada ensamble, una con la anotaci√≥n con *eggnog* e otra con *kegg*. 

### 11.3 Prokka

[**Prokka**](https://github.com/tseemann/prokka) √© uma ferramente amplamente usada para anota√ß√£o funcional em genomas.

#### 11.3.1. Instala√ß√£o

Siga os seguintes comandos para a instala√ß√£o de **Prokka**:

```
# Crie um ambiente e instale prokka
conda create -y -n prokka prokka==1.14.6

# Ative o ambiente
conda activate prokka

# Instale alguma dependencias adicionais
conda install -y perl-app-cpanminus
cpanm Bio::SearchIO::hmmer --force
```

#### 11.3.2 Uso

**Prokka** tem diferentes n√¨veis de comandos segundo a expertise do usu√°rio (iniciante, moderado, especialista, experto, mago, etc). Para este tutorial usaremos o n√≠vel moderado. 

O comando para anotar genoma por genoma √©:

> üá™üá∏
**Prokka** tiene diferentes n√≠veles de comando seg√∫n la experticia del usuario (iniciante, moderado, especialista, experto, mago, etc). Para este tutorial usaremos el n√≠vel moderado.
>
El comando para anotar genoma por genoma

```
prokka --outdir 19.Prokka -prefix MAG10 13.MAGS/HQ_MQ_MAGs/MAG10.fa --cpus 6
```

Ou em loop para automatizar a anota√ß√£o de todos seus genomas:


```
for i in 13.MAGS/HQ_MQ_MAGs/*.fa
do
BASE=$(basename $i .fa)
prokka --outdir 19.Prokka --prefix ${BASE} $i --cpus 6 --force
done
```

**SINTAXE**

```
prokka --outdir output/ --prefix prefix genome.fa --cpus xx --force
```

* `--outdir`: Diret√≥rio de sa√≠da
* `--prefix`: prefijo para os outputs
* `genome.fa`: caminho para o genoma em formato fasta
* `--cpus`: n√∫mero de n√∫cleos/threads
* `--force`: for√ßar o programa a sobre escrever se a pasta j√° estiver criada.

**Outputs**

* `.gff`: anota√ß√µes em formato GFF3, com as sequ√™ncias e as anota√ß√µes, pode ser visualizado no programa Artemis.
* `.gbk`: arquivo Genbank derivado do `.gff`. Se o input foi um arquivo multifasta, este ser√° um arquivo multi-Genbank
* `.fna`: sequ√™ncias input em nucleot√≠deos 
* `.fna`: arquivo de pote√≠nas com a transla√ß√£o dos CDS
* `.ffn`: arquivo fasta com todas os transcritos preditos (CDS, rRNA, tRNA, tmRNA, misc_RNA)
* `.sqn`: anota√ß√µes formato Sequin
* `.txt`: Estat√≠sticas relacionadas com as *features* anotadas
* `.tsv`: tabela com todas as *features* anotadas (locus_tag, ftype, len_bp, gene, EC_number, COG, product)


**Nota:** Para mais informa√ß√µes visite o [GitHub](https://github.com/tseemann/prokka).

#### 11.4. Manipula√ß√£o de Dados




---

## Em constru√ß√£o...
