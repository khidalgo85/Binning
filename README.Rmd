---
output:
    github_document:
    pandoc_args: --webtex
always_allow_html: true
---


<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "imgs/",
  out.width = "100%"
)

knitr::opts_chunk$set(echo = TRUE)
```


<!-- badges: start -->
![forthebadge](https://img.shields.io/badge/GEMM-Building-orange)
![forthebadge](https://forthebadge.com/images/badges/built-with-science.svg)
<!-- badges: end -->

# Reconstru√ß√£o de Genomas - Binning! <img src="imgs/1.png" align="right" width = "120px"/>
**Developer: Victor Borin Centurion - Kelly Hidalgo**

Pipeline para reconstru√ß√£o de genomas a partir de metagenomas, usando v√°rias ferramentas de *binning* e *DAS tools* para integrar os resultados dos algoritmos de binning para calcular um conjunto otimizado e n√£o redundante de MAGs (*Metagenome Assembled Genomes*) de uma √∫nica montagem.


## Ferramentas bioinform√°ticas
<font size=2> Para instala√ß√£o de ferramentas bioinform√°ticas vai [aqui]() </font>

### Controle de qualidade

- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)

- [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)

### Assembly

- [Megahit](https://github.com/voutcn/megahit) (v1.2.4-beta)

### Mapping

- [Bowtie2](https://github.com/BenLangmead/bowtie2) (v2.3.4.3)

- [Samtools](https://github.com/samtools/samtools) (v1.9)

### Binning

- [Metabat2](https://bitbucket.org/berkeleylab/metabat/src/master/) (v2.12.1) 

- [Metabat1](https://bitbucket.org/berkeleylab/metabat/src/master/)

- [CONCOCT](https://github.com/BinPro/CONCOCT) (v1.1.0-0)

- [MaxBin](https://denbi-metagenomics-workshop.readthedocs.io/en/latest/binning/maxbin.html) (v2.2.6) 

- [BinSanity](https://github.com/edgraham/BinSanity) (v0.3.4-0)

- [Vamb](https://github.com/RasmussenLab/vamb)(v3.0.2)

### Controle de qualidade

- [Ultrabinner](https://github.com/Huangpq2019/UltraBinner) para formatar os Bins como input para DasTools

- [CheckM](https://github.com/Ecogenomics/CheckM/wiki) (v1.1.2-1)

- [DasTools](https://github.com/cmks/DAS_Tool) (v1.1.2-0 - efesto) (Diamond v0.9.30.131)

### Refinamento

- [MagPurify2](https://apcamargo.github.io/magpurify2/docs/)(v2.1)

---

## Processamento

O *binning* √© uma t√©cnica bioinform√°tica para reconstruir genomas a partir de metagenomas. O processamento inclui as seguintes fases: **1) Sequenciamento de metagenomas**, do qual vai se obter reads; **2) Controle de qualidade das reads**, usando ferramentas como *Trimmomatic* s√£o filtradas as reads de baixa qualidade; **3) Montagem das reads**, dos quais v√£o se obter contigs ou scafolds; **4) Mapeamento das reads**, isto com o objetivo de saber a abund√¢ncia e o origem de cada read (informa√ß√£o necess√°ria para a seguinte etapa); **5)** ***Binning***, usando diferentes ferramentas/algoritmos (i.e. *Metabat2, MaxBin, CONCOCT, BinSanity*) que clusterizam os contigs/scalfolds baseado em diferentes caracter√≠sticas similares tais como: n√≠veis de cobertura de cada contig/scalfold, genes marcadores de c√≥pia √∫nica, perfil de frequ√™ncias de tetranucleot√≠deos, formando os genomas reconstruidos tamb√©m conhecidos como *MAGs* ou *Bins*; **6) Controle de qualidade**, compreende o uso de ferramentas bioinform√°ticas (i.e. *CheckM, DAStool*) para √° analise da qualidade de cada um dos bins gerados, em √≠tems como completude e contamina√ß√£o; **7) Anota√ß√£o taxon√¥mica** e **8) Anota√ß√£o funcional**

A continua√ß√£o uma vis√£o geral do processo de **Binning**

<img src="imgs/binning.png" align="center" width = "100%"/>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

> **O que vai encontrar aqui:**
>
> * Controle de qualidade de sequenciamento shotgun 
>
> * Montagem de metagenomas 
>
> * Mapeamento de reads 
>
> * Reconstru√ß√£o de genomas 
>
> * Anota√ß√£o taxon√¥mica e funcional de genomas  

</div>

---
---

### 1. Sequenciamento massivo de metagenomas

O sequenciamento massivo ou em larga escala de metagenomas, compreende a extra√ß√£o do DNA total de uma amostra de qualquer ambiente ou tipo de amostra (i.e. solo, agua, feces, esponjas marinas, tecidos vegetais, etc) e o sequenciamento por uma t√©cnica chamada *shotgun*. Onde o DNA total √© fragmentado em muitos pedacinhos, os quais s√£o sequenciados aleatoriamente, obtendo assim as chamadas *reads*, que s√£o sequ√™ncias pequenas. 

<img src="imgs/seq1.png" align="center" width = "100%"/>

---

### 2. Dataset

Primeiro usando comandos b√°sicos de **UNIX** (tutorial [aqui]()), crie um diret√≥rio chamado `binning` desde o qual v√£o ser rodados todos os comandos
```
mkdir binning 
cd binning/
```
Agora dentro de binning crie outro diret√≥rio chamado `00.RawData`, onde vai descarregar o dataset de exemplo para este tutorial

```{bash}
mkdir 00.RawData
rm -r 00.RawData
```

Para descarregar o dataset...
```
curl -L https://ndownloader.figshare.com/files/12389045 -o dataset.tar.gz
tar -xzvf dataset.tar.gz
rm dataset.tar.gz
cd metagen_tut
```
Com `ls`voc√™ pode ver o conte√∫do descarregado. T√™m tr√™s pastas `data`, `results` e `working`. Elimine as duas √∫ltimas pastas e o conte√∫do da pasta `data` coloque na pasta `00.RawData`.

```{zsh eval=FALSE}
rm -r metagen_tut/results metagen_tut/working
mv metagen_tut/data/* 00.RawData/
rm -r metagen_tut
ls 00.RawData
```
Por √∫ltimo "listou" (`ls`) o conte√∫do da pasta `00.RawData`, vai observar que t√™m 4 amostras paired-end (R1 e R2) `Sample_A_1.fastq.gz    Sample_A_2.fastq.gz   Sample_B_1.fastq.gz   Sample_B_2.fastq.gz   Sample_C_1.fastq.gz   Sample_C_2.fastq.gz   Sample_D_1.fastq.gz Sample_D_2.fastq.gz`

---

### 3. Controle de qualidade

**Checagem da qualidade**

Estando no diret√≥rio `binning`, crie um diret√≥rio chamado `01.FastqcReports`

```
mkdir 01.FastqcReports
```
A qualidade das reads deve ser checada com a ferramenta *FASTQC*. Se a ferramenta foi instalada pelo conda, lembre de ativar previamente o ambiente onde foi instalada

```
fastqc -t 10 00.RawData/* -o 01.FastqcReports/
``` 

Dentro da pasta `01.fastqcReports/` foram gerados uma s√©rie de arquivos `.html`que correspondem aos reportes de qualidade das sequ√™ncias de cada amostra.

D√™ uma olhada em cada um deles e avalia a qualidade de suas sequ√™ncias. O ideal √© que o *Phred Score* seja maior de 30, como minimo 20. Se necess√°rio trimar as sequ√™ncias com qualidade baixa.

Veja um exemplo üëá de uma amostra paired-end

```{r B63_1, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 1 FORWARD"}
knitr::include_graphics("imgs/B63_1.png")
```

```{r B63_2, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 1 REVERSE"}
knitr::include_graphics("imgs/B63_2.png")
```

√à comum que as sequ√™ncias reverse tenham uma qualidade inferior √†s forward. Mas em termos gerais a amostra do exemplo tem uma qualidade aceit√°vel. 

Veja um exemplo de uma amostra com qualidade baixa üëá

```{r PM63_1, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 2 FORWARD"}
knitr::include_graphics("imgs/PM63_1.png")
```

```{r PM63_2, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 2 REVERSE"}
knitr::include_graphics("imgs/PM63_2.png")
```

A qualidade da amostra 2 n√£o √© toleravel para as an√°lises seguintes. Mesmo que a amostra 1 tenha uma qualidade aceit√°vel, pode ser melhorada com a trimagem. 

**Trimagem**

Estando na pasta `binning` crie uma nova chamada `02.CleanData` para armazenar as sequ√™ncias "limpas" e outra chamada `unpaired` para guardar as sequ√™ncias n√£o pareadas de cada amostra.

```
mkdir 02.CleanData
mkdir unpaired
```

A ferramenta a usar para a trimagem das sequencias √© **Trimmomatic**.
Depois de revisar os reportes de qualidade e observar que todas as amostras podem ser processadas com os mesmos param√™tros de corte, pode ser usado um loop para facilitar o processo e um comando s√≥ processar todas as amostras.

```
for i in 00.RawData/*1.fq.gz 
do
BASE=$(basename $i 1.fq.gz)
trimmomatic PE -threads 20 $i  00.RawData/${BASE}2.fq.gz 02.CleanData/${BASE}1_paired.fq.gz unpaired/${BASE}1_unpaired.fq.gz 02.CleanData/${BASE}2_paired.fq.gz unpaired/${BASE}2_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:150
done
```

**Revisar a qualidade de novo**

```
fastqc -t 20 02.CleanData/* -o 01.FastqcReports/
```
Amostra depois da trimagem üëá

```{r B63_1_paired_trimmed, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 1 FORWARD"}
knitr::include_graphics("imgs/B63_1_trimmed.png")
```

```{r B63_2_paired_trimmed, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 1 REVERSE"}
knitr::include_graphics("imgs/B63_2_trimmed.png")
```

```{r PM63_1_paired_trimmed, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 2 FORWARD"}
knitr::include_graphics("imgs/PM63_1_trimmed.png")
```

```{r PM63_2_paired_trimmed, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Amostra 2 REVERSE"}
knitr::include_graphics("imgs/PM63_2_trimmed.png")
```
Veja a diferen√ßa da qualidade depois da trimagem, melhorou muito n√©? üòÄ
No entanto tem que ser avaliado com cuidado o n√∫mero de reads perdidas.
Para estes exemplo teve uma perda de 20% e 28% respeitivamente. As porcentagens s√£o boas para avaliar, no entanto se o n√∫mero de sequ√™ncias de entrada for muito baixo, tem que balancear entre poucas reads com alta qualidade, ou mais reads com uma qualidade apenas aceit√°vel. 

---

### 4. Montagem

Para montar as reads vai ser usado Megahit. Este programa vai usar algoritmos para construir *contigs* apartir das sequ√™ncias de todas as amostras. *Contigs* ent√£o, s√£o sequencias mais longas, ou conjunto de reads.

O modo a usar √© de co-assembly, que significa que vai ser feito uma montagem s√≥ com todas as amostras. Para isto primeiramente devem ser unidos as sequ√™ncias F ou 1 de todas amostras em um arquivo s√≥, e o mesmo dever ser feito para as sequ√™ncias 2 ou R.

**Concatenar as sequ√™ncias das amostras**

Crie uma pasta para armazenar as sequencias concatenadas.

```
mkdir 03.MergeData
```

Usando o comando `cat`, concatene as sequ√™ncias forward de todas as amostras.

```
cat 02.CleanData/*_1_* > 03.MergeData/samples_1.fq.gz
```
Lembre que o **"*"** significa que √© tudo. Ou seja no caso acima, concatene as amostras que est√£o dentro da pasta `02.CleanData/`, qualquer que seja o come√ßo do nome, mas que depois leve um "_1_" (sequ√™ncias 1 ou Forward) e depois qualquer que seja o fim do nome, e crie um arquivo chamado "samples_1.fq.gz" dentro da pasta `03.MergeData/`.

Agora fa√ßa o mesmo procedimento para as sequ√™ncias 2 ou reverse.

```
cat 02.CleanData/*_2_* > 03.MergeData/samples_2.fq.gz
```

**Montagem**

Crie uma pasta para armazenar a sa√≠da da montagem.

```
mkdir 04.Assembly
```
Se o Megahit estiver instalado pelo Conda, tem que ser ativado o ambiente onde est√° instalada a ferramenta.

O tamanho m√≠nimo de contig a usar ser√° de 500.

Com o intuito de melhorar a montagem, as reads unpaired (reads que n√£o foram pareadas quando foi feita a trimagem que se encontram na pasta `unpaired/`) podem ser usadas. Para isto o procedimento de concatenar os arquivos deve ser feito, com a diferen√ßa que desta vez vai ser criado um arquivo s√≥ com **TODAS** as sequ√™ncias.

```
cat unpaired/* > 03.MergeData/unpaired.fq.gz
```

Para a montagem propiamente dita, o comando se encontra abaixo üëáüèø

```
nohup megahit -1 03.MergeData/samples_1.fq.gz -2 03.MergeData/samples_2.fq.gz -r 03.MergeData/unpaired.fq.gz --presets meta-large --memory 0.95 --num-cpu-threads 20 -o 04.Assembly1/ --min-contig-len 500 -t 20
```

Se voc√™ quiser explorar as op√ß√µes do programa, consulte o manual ou digite o comando `megahit --help`.

O `nohup` no inicio do comando √© um programinha que vai guardando em um arquivo de texto todo o processo do comando. Depois voc√™ pode aceder a ele com o comando `nano nohup.out`.

**Qualidade da montagem**

A qualidade da montagem √© avaliada por um programa chamado *MetaQuast*. Ele gera um reporte completo com todos os dados da montagem. √â muito √∫til quando voc√™ quer comparar varias montagens com diferentes par√¢metros, das mesmas amostras, para assim escolher o melhor.

Lmebre de ativar o ambiente do Conda onde o *MetaQuast* estiver instalado

```
nohup metaquast.py 04.Assembly/final.contigs.fa -o 05.AssemblyQuality
```

---

### 5. Mapping

Agora √© preciso fazer o mapeamento as reads originais dentro do co-assembly para obter informa√ß√µes de cobertura para cada contig em cada amostra. O programa para isto se chama *Bowtie2*.

Primeiro crie uma pasta para armazenar a sa√≠da do mapeamento

```
mkdir 06.Mapping
```

O primeiro passo do mapeamento √© criar um √≠ndice de nosso co-assembly

```
nohup bowtie2-build 04.Assembly/final.contigs.fa 06.Mapping/final_assembly_DB
```
Agora vamos a mapear as reads das amostras individuais no co-assemblys. O processo pode ser feito amostra por amostra, o podemos usar um loop para fazer todas as amostras ao mesmo tempo. **Cuidado:** Fique atento a nome de suas amostras, e se for necess√°rio modifique o comando, para que se ajuste as suas amostras. 

```
for f in 02.CleanData/*_1.fq.gz; do r=${f%_*}_2.fq.gz; BASE=${f##*/}; SAMPLE=${BASE%_*}; bowtie2 -q -1 $f -2 $r -x 06.Mapping/final_assembly_DB -p 10 -S 06.Mapping/${f%_*}.sam; done
```
A linha de comando para cada amostra √©:

```
bowtie2 -q -1 02.CleanData/B52_1.fq.gz -2 02.CleanData/B52_2.fq.gz -x 06.Mapping/final_assembly_DB -p 10 -S 06.Mapping/B52.sam
```

Agora √© necess√°rio converter os arquivos `.sam` para `.bam`. Tamb√©m ser√° feito o comando usando um loop

```
cd 06.Mapping/
for f in *.sam; do filename="${f%%.*}"; samtools view -@ 10 $f -b > ${filename}.bam; done
```
O comando individual √©:

```
samtools view -b -o 06.Mapping/B52.bam 06.Mapping/B52.sam
```
Ap√≥s transformar em arquivo `.bam`, devem ser ordenados.

```
for f in *.bam; do filename="${f%%.*}"; samtools sort -@ 10 $f > ${filename}.sorted.bam; done

ls ## conferir que estejam os arquivos

```

E por √∫litmo os arquivos v√£o ser indexados

```
for f in *.sorted.bam; do filename="${f%%.*}"; samtools index -@ 10 $f > ${filename}.index.bam; done

samtools index -@ 10 B52.sorted.bam B52.index.bam

## para voltar na pasta raiz 02.Binning/
cd ..
```

---

### 6. Binning

Para a reconstru√ß√£o dos genomas, ser√£o usadas v√°rias ferramentas para ter um n√∫mero maior de MAGs recuperados.

#### 6.1. MetaBat2

Crie uma pasta para armazenar a sa√≠da do processamento em *MetaBat2*

```
mkdir 07.MetaBat2
```

Usando os arquivos ordenados `.sorted..bam` vai gerar um arquivo `.txt` com a informa√ß√£o da cobertura, necess√°ria para a recupera√ß√£o dos genomas.
Como sempre lembrando que precisa ativar o ambiente onde se encontra instalado o *MetaBat2*

```
jgi_summarize_bam_contig_depths --outputDepth 07.MetaBat2/Depth.txt 06.Mapping/*sorted.bam
```
O *MetaBat2* tem v√°rios par√¢metros para customizar, o tamanho minimo de contig √© o mais comum de ser modificado. Neste pipeline voc√™ vai encontrar tr√™s rodadas com *MetaBat2* com diferentes tamanhos minimos de contigs.

**First Trial**

Crie um diret√≥rio dentro da pasta `07.MetaBat2` chamado `01.FirstTrial`

```
mkdir 07.MetaBat2/01.FirstTrial
```

Para este primeiro trial o tamanho minimo de contig ser√° de 1500

```
nohup metabat2 -i 04.Assembly/final.contigs.fa -a 07.MetaBat2/Depth.txt -m 1500 -t 10 -o 07.MetaBat2/01.FirstTrial/Metabat2_firsttrial_
```

Para o segundo trial o tamanho minimo de contig ser√° de 2500, que √© o default da ferramenta, por isso n√£o precisa colocar o flag `-m`.

```
mkdir 07.MetaBat2/02.SecondTrial

nohup metabat2 -i 04.Assembly/final.contigs.fa -a 07.MetaBat2/Depth.txt -t 10 -o 07.MetaBat2/02.SecondTrial/Metabat2_secondtrial_
```

Por √∫ltimo, para terceira rodada, ser√£o modificados mais par√¢metros.
Para conhecer todos os par√•metros que podem ser customizados, digite o comando `metabt2 --help`. Com o flag `-m` ou `--minContig`, como j√° foi usado nas rodadas anteriores √© poss√≠vel modificar o tamanho m√≠nimo dos contigs, para este caso ser√° usado 3000. Com o flag `--maxEdges`, pode-se modificar o m√°ximo n√∫mero de *edges* (arestas) por n√≥. Entre maior seja o n√∫mero, o algoritmo √© mais sensitivo. O default √© 200, vai ser usado 500. O flag `--minS` modifica o socre m√≠nimo de cada *edge*, entre maior seja √© mais espec√≠fico. O default √© 60, vai ser usado 80.
Ent√£o o comando √© o seguinte:

```
mkdir 07.MetaBat2/03.ThirdTrial

nohup metabat2 -i 04.Assembly/final.contigs.fa -a 07.MetaBat2/Depth.txt -t 10 --minContig 3000 --minCV 1.0 --minCVSum 1.0 --minS 80 --maxEdges 500 -o 07.MetaBat2/03.ThirdTrial/Metabat2_thirdtrial_
```

#### 6.2. CONCOCT

A seguinte ferramenta de reconstru√ß√£o de genomas √© *CONCOCT*. Ative o ambiente do Conda onde est√° instalada a ferramenta. 

Crie uma pasta para armazenar os arquivos de sa√≠da do *CONCOCT*

```
mkdir 08.CONCOCT
```
Primeiramente v√£o ser cortados os contigs em partes menores
```
cut_up_fasta.py 04.Assembly/final.contigs.fa -c 10000 -o 0 --merge_last -b 08.CONCOCT/contigs_10K.bed > 08.CONCOCT/contigs_10K.fa
```

Agora ser√° gerada uma tabela com a informa√ß√£o da cobertura por amostra e subcontig, usando os arquivos `.sorted.bam`.
```
concoct_coverage_table.py 08.CONCOCT/contigs_10K.bed 06.Mapping/*.sorted.bam > 08.CONCOCT/coverage_table.tsv
```

Rode o CONCOCT

```
concoct --composition_file 08.CONCOCT/contigs_10K.fa --coverage_file 08.CONCOCT/coverage_table.tsv --length_threshold 1500 --threads 30 -b 08.CONCOCT/
```
Agora mesclar o agrupamento contig no agrupamento dos contigs originais

```
merge_cutup_clustering.py 08.CONCOCT/clustering_gt1500.csv > 08.CONCOCT/clustering_merged.csv
```
Extrair os bins como arquivos individuais FASTA

```
mkdir 08.CONCOCT/bins

extract_fasta_bins.py 04.Assembly/final.contigs.fa 08.CONCOCT/clustering_merged.csv --output_path 08.CONCOCT/bins
```
#### 6.3. MaxBin

A terceira ferramenta √© chamada *MaxBin*. 

Primeiro crie a pasta para sa√≠da do *MaxBin*

```
mkdir 09.MaxBin
```

Para obter a informa√ß√£o da cobertura s√£o usados os arquivos `.sam`. Para facilitar primeiro entre na pasta anteriormente criada 

```
cd 09.MaxBin/
```
Copie os arquivos gerados no mapeamento (`.sam`) que se encontram na pasta `06.Mapping/` para a pasta atual (`09.MaxBin/`)
```
cp ../06.Mapping/*.sam ./
```
Gere os arquivos de cobertura
```
for f in *.sam; do pileup.sh in=$f out=${f%}.txt; done

cd .. ## sair da pasta para ficar na pasta base 09.MaxBin/
```
Depois gere um arquivo com os nomes e caminhos dos arquivos de cobertura gerados acima
```
ls 09.MaxBin/*sam.txt > 09.MaxBin/abundance.list
```
A continua√ß√£o o comando para gerar os bins com *MaxBin*

```
run_MaxBin.pl -contig 04.Assembly/final.contigs.fa -abund_list 09.MaxBin/abundance.list -max_iteration 20 -min_contig_length 1500 -thread 10 -out 09.MaxBin/bins/maxbin
```
*MaxBin* gera os MAGs com extens√£o `.fasta`, mas para facilitar as an√°lises downstream e padronizar a mesma extens√£o dos bins gerados por todas as ferramentas, √© melhor converter eles para `.fa`.

Para isto vamos a usar um loop for, para realizar o procedimento com todos bins de uma vez s√≥.

```
cd 09.MaxBin/

for file in *.fasta
do mv "$file" "$(basename "$file" .fasta).fa"
done

ls ## para conferir que agora todos os bins terminam em .fa

cd ../ # para voltar √† pasta base
```

#### 6.4. BinSanity

A quarta e √∫ltima ferramenta se chama *BinSanity*.
Como sempre crie uma pasta para a sa√≠da do processamento nesta ferramenta.
```
mkdir 10.BinSanity
```

Gere a informa√ß√£o da cobertura das amostras
```
Binsanity-profile -i 04.Assembly/final.contigs.fa -s 06.Mapping/ -T 10 -c 10.BinSanity/coverage_profile.txt -o 10.BinSanity/
```
No seguinte comando ser√£o gerados os bins
```
Binsanity-lc -f . -l 04.Assembly/final.contigs.fa --kmean_threads 10 -x 1500 --checkm_threads 10 --Prefix binsanity_ -c 10.BinSanity/coverage_profile.txt.cov.x100.lognorm
```
*BinSanity* criou uma pasta com onde est√£o todos os resultados da corrida `BINSANITY-RESULTS/` e os bins se encontram em  `BINSANITY-RESULTS/binsanity_-KMEAN-BINS/`

Ao igual que com *MaxBin* tem que converter os bins para `.fa`, com a diferen√ßa que o *BinSanity* gera os MAGs com extens√£o `.fna`.

```
cd BINSANITY-RESULTS/binsanity_-KMEAN-BINS/

for file in *.fna
do mv "$file" "$(basename "$file" .fna).fa"
done

ls ## para conferir que agora todos os bins terminam em .fa

cd ../../ # Para voltar √† pasta base
```

#### 6.5. Vamb

A quinta ferramenta de binning que vai ser usada √© chamada *Vamb* (*Variational autoencoder for metagenomic binning*). Esta ferramenta usa a informa√ß√£o da composi√ß√£o das sequencias dos contigs e a informa√ß√£o de co-abund√¢ncia dos arquivos `.bam` para clusterizar em bins.

Para facilitar o processo, pode ser usado o arquivo `07.Metabat2/Depth.txt` gerado para rodar o *Metabat2* com o comando `--jgi_summarize_bam_contigs_depths`.

```
vamb --outdir 11.Vamb --fasta 04.Assembly/final.contigs.fa --jgi 07.MetaBat2/Depth.txt --minfasta 200000 -m 500
```
Ao igual que o *BinSanity*, o *Vamb* gera os bins com extens√£o `.fna`, por tanto √© necess√°rio converter eles para `.fa`.

```
cd 11.Vamb/bins/

for file in *.fna
do mv "$file" "$(basename "$file" .fna).fa"
done

ls ## para conferir que agora todos os bins terminam em .fa

cd ../../ # Para voltar √† pasta base
```
---

### 7. Desreplica√ß√£o com DAS TOOL

o *DAS Tool* √© uma ferramenta que integra os resultados de diferentes ferramentas de reconstru√ß√£o de genomas (Binning) para determinar o conjunto otimizado de MAGs, n√£o redundantes de uma √∫nica montagem.
```
mkdir 12.DasTool
```
Nem todas as ferramentas de binning fornecem resultados em formato de tabela `.tsv`, com o contigs IDs e os bin IDs. Para formatar isto o *DAS Tool* tem um script adicional `scaffolds2bin` que converte um conjunto de bins no formato `.fasta`em um arquivo tabular para ser usado como input do *DAS Tool*. Salve as tabelas na pasta criada para *DAS Tool*.
```
cd 07.MetaBat2/01.FirstTrial/ #entrar na pasta da primeira rodada com o MetaBat2

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_firsttrial.tsv #Script pra criar a tabela

cd ../02.SecondTrial/ 

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_secondtrial.tsv

cd ../03.ThirdTrial/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/metabat2_thirdtrial.tsv

cd ../../08.CONCOCT/bins/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/concoct.tsv

cd ../../09.MaxBin/

Fasta_to_Scaffolds2Bin.sh -e fa > ../12.DasTool/maxbin.tsv

cd ../BINSANITY-RESULTS/binsanity_-KMEAN-BINS/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/binsanity.tsv

cd ../../11.Vamb/bins/

Fasta_to_Scaffolds2Bin.sh -e fa > ../../12.DasTool/vamb.tsv
```
Ap√≥s criadas as tabelas dos bins de cada ferramenta usada, √© rodado o *DAS Tool*
```
cd ../../12.DasTool/

DAS_Tool -i vamb.tsv,binsanity.tsv,concoct.tsv,maxbin.tsv,metabat2_firsttrial.tsv,metabat2_secondtrial.tsv,metabat2_thirdtrial.tsv -l vamb,binsanity,concoct,maxbin,metabat2ft,metabat2st,metabat3tt -c ../04.Assembly/final.contigs.fa -t 10 -o ./ --score_threshold 0.5 --write_bins 1 --search_engine diamond
```
Ap√≥s terminar a corrida do *DAS Tool*, descarregue os arquivos:
* `_DASTool_hqBins.pdf`
* `_DASTool_summary.txt`
* `_DASTool_scores.pdf`

Para descarregar arquivos desde o servidor para seu computador o comando usado √© `scp`. Aqui um exemplo::
```
scp usuario@ip.do.seu.servidor:/caminho/do/arquivo caminho/no/seu/computador/
```
Esse comando deve ser rodado no terminal do **seu computador**.

Explore os arquivos descarregados. O `_DASTool_hqBins.pdf` √© um gr√°fico de barras que mostra o n√∫mero de bins gerados por cada ferramenta e os bins que passaram o filtro definido no *DAS Tool*, para nosso caso score > 0.5, diferenciando por tons de azul o n√∫mero de bins de completude >90%, >80%, >70%, >%60.

```{r DAS Tool hq bins, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Gr√°fico qualidade dos bins - DASTools"}
knitr::include_graphics("imgs/DASTool_quality.png")
```

O `_DASTool_scores.pdf`, √© um gr√°fico de distribui√ß√£o dos bins de cada ferramenta pelo score. Entre maior seja a √°rea de distribui√ß√£o para cima √© melhor, porque significa que maior quantidade de MAGs com score pr√≥ximo de 1 foram gerados. 

```{r DAS Tool scores bins, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Gr√°fico de score dos bins - DASTools"}
knitr::include_graphics("imgs/DASTool_scores.png")
```

Por √∫ltimo `_DASTool_summary.txt`, √© uma tabela com muitas informa√ß√µes dos bins que passaram o filtro do *DAS Tool* (p.e. "size", "contigs", "N50", "binScore", "SCG_completeness",	"SCG_redundancy")

```{r DAS Tool tabela, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.cap="Tabela DASTools"}
knitr::include_graphics("imgs/tabela.png")
```

Adicionalmente, o **DAS Tool** separa os bins que passaram o *threshold* na pasta `12.DasTool/_DASTool_bins/`. 
---

### 8. Qualidade dos MAGs

A qualidade dos MAGs √© avaliada usando uma ferramenta chamada *CheckM*. Basicamente a avalia√ß√£o consiste em comparar os MAGs com uma base de dados de genes de c√≥pia √∫nica para assim saber que t√£o completo e contaminado est√° cada um dos genomas recuperados. 

Ser√£o analisados os genomas desreplicados da sa√≠da do **DAS Tool** (`12.DasTool/_DASTool_bins/`)

Agora crie uma pasta para armazenar a sa√≠da do *CheckM*:
```
mkdir 13.CheckM
```
Para rodar o *CheckM* √© preciso criar um diret√≥rio para os arquivos temporais que ser√£o criados enquanto a corrida.
```
mkdir tmp
```
Lembre **SEMPRE** que **TODA** ferramenta tem um men√∫ de ajuda (`-h` ou `--help`).

Para rodar a an√°lise de qualidade pelo *CheckM* use o seguinte comando:
```
checkm lineage_wf 12.DasTool/_DASTool_bins/ 13.CheckM/ -t 10 -x fa --tmpdir tmp --tab > 13.CheckM/output.txt
```
---

### 9. Refinamento com MagPurify2

**MagPurify2** √© uma ferramenta que auxilia no refinamento dos MAGs, eliminando contamina√ß√£o, baseado nas caracteristicas gen√¥micas dos contigs, usados para a clusteriza√ß√£o. Essas caracter√≠sticas s√£o: **(1)** frequ√™ncias de tetranucleotideos, **(2)** conte√∫do de GC, **(3)** coverage e **(4)** genome codon usage.

Basicamente o programa analisa todos os MAGs, procurando divergencias entre os contigs de cada um repeito √†s caracteristicas gen√¥micas, para assim detectar contigs contaminantes, os quais na √∫ltima etapa ser√£o eliminados dos bins, purificando os genomas. 

Esta ferramenta tem um recurso muito interessante, que consiste em usar a tabela de sa√≠da do **CheckM**, com a qual ela vai ajustar os **cut-off** dependendo da qualidade medida pelo *CheckM**. 

Para gerar essa tabela, √© necess√°rio rodar um comando do **CheckM**.

```
checkm qa --threads 10 --tab_table --file 13.CheckM/checkm_output.tsv 13.CheckM/lineage.ms 13.CheckM/
```
Ap√≥s obtida a tabela, pode ser rodado o **MagPurify2**. Voc√™ pode rodar cada m√≥dulo por separado um atr√°s do outro, com uma linha de comando para cada um. (consulte `magpurify --help`)

> **M√≥dulos**
- composition         Identify putative contaminants using tetranucleotide frequencies and GC content.
 - coverage            Identify putative contaminants using coverage profiles.
 - codon_usage         Identify putative contaminants using gene codon usage profiles.
 - taxonomy            Identify putative contaminants through taxonomy assignment.
 - filter              Remove identified contaminants from input MAGs)

Ou usar o modo end_to_end, que vai rodar todos os m√≥dulos, tamb√©m um tr√°s do outro, mas de maneira autom√°tica, com uma linha de comando s√≥, que se encontra a continua√ß√£o: 
```
magpurify2 end_to_end 12.DasTool/_DASTool_bins/* 14.MagPurify2 15.MagPurify_filtered_bins --bam_files 06.Mapping/B52.sorted.bam 06.Mapping/B63.sorted.bam 06.Mapping/B65.sorted.bam 06.Mapping/PM62.sorted.bam 06.Mapping/PM63.sorted.bam 06.Mapping/PM65.sorted.bam --fast_mode -t 10 --checkm_file 13.CheckM/checkm_output.tsv
```
Na linha de comando anterior foi usado o modo r√°pido (`--fast_mode`), no qual n√£o √© feita a predi√ß√£o de genes nem a clasifica√ß√£o taxon√¥mica. Para rodar o modo completo o comando √© o seguinte:

```
magpurify2 end_to_end 12.DasTool/_DASTool_bins/* 16.MagPurify2_complete 17.MagPurify_filtered_bins_completed --bam_files 06.Mapping/B52.sorted.bam 06.Mapping/B63.sorted.bam 06.Mapping/B65.sorted.bam 06.Mapping/PM62.sorted.bam 06.Mapping/PM63.sorted.bam 06.Mapping/PM65.sorted.bam --taxonomy_database /home/bioinfo/Documentos/Databases/MagPurify/magpurify2DB -t 10 --checkm_file 13.CheckM/checkm_output.tsv
```
Ap√≥s o refinamento √© necess√°rio rodar novamente o *CheckM*, para conferir as melhoras na contamina√ß√£o dos MAGs.


**Magpurify2** `--fastmode`
```
mkdir 18.CheckMFiltered

checkm lineage_wf 15.MagPurify_filtered_bins/ 18.CheckMFiltered/ -t 10 -x fna --tmpdir tmp --tab > 18.CheckMFiltered/output_filtered.txt

checkm qa --threads 10 --tab_table --file 18.CheckMFiltered/checkm_filtered_output.tsv 18.CheckMFiltered/lineage.ms 18.CheckMFiltered/
```

**Magpurify2** `complete mode`
```
mkdir 19.CheckMFiltered_complete

checkm lineage_wf 17.MagPurify_filtered_bins_completed/ 19.CheckMFiltered_complete -t 10 -x fna --tmpdir tmp --tab > 19.CheckMFiltered_complete/output_filtered.txt

checkm qa --threads 10 --tab_table --file 19.CheckMFiltered_complete/checkm_filtered_complete_output.tsv 19.CheckMFiltered_complete/lineage.ms 19.CheckMFiltered_complete/
```
Descarregue as tabelas do *CheckM* de antes e depois do refinamento com *MagPurify2* e compare os resultados.

Para comparar os resultados dos **score** dos bins, de antes vs depois do refinamento foram constru√≠dos *scriptcharts* com o pacote **ggplot2** de R. Script [aqui](https://github.com/khidalgo85/Binning/blob/master/binning.R)

**Score**

<img src="https://render.githubusercontent.com/render/math?math=Score = Completeness - (5 *  Contamination)">



```{r low, echo=FALSE, out.width="30%", out.height="30%"}
knitr::include_graphics("imgs/comp_score_low.png")
knitr::include_graphics("imgs/comp_score_medium.png")
knitr::include_graphics("imgs/comp_score_high.png")
```

 
### 10. Anota√ß√£o Taxon√¥mica
Existem diversas ferramentas para anota√ß√£o taxon√¥mica, no entanto a mais utilizada em estudos com MAGs √© **GTDB-Tk**. O qual √© um software criado para asigna√ß√£o taxon√¥mica de genomas de bact√©rias e arqueias baseado no **Genome Database Taxonomy - GTDB**. Dentro desta base de dados existe uma grande quantidade de MAGs obtidos de amostras ambientais.

A anota√ß√£o taxon√¥mica com **GTDB-Tk** precisa de ~152 gb de mem√≥ria RAM, o qual √© um gasto computacional muito alto. No entanto, esta ferramenta se encontra na plataforma [KBase](https://www.kbase.us/), na qual √© rodado em um servidor p√∫blico, sem ter que se preocupar pelas especifica√ß√µes de sua maquina.

O primeiro passo √© criar uma conta [aqui](https://narrative.kbase.us/#signup). Depois explore as funcionalidades da plataforma e aprenda a us√°-la, aqui.

Enquanto isso, desde o servidor descarregue seus bins (`12.DasTool/_DASTool_bins/`). No **KBase**, crie uma nova *narrativa*. Depois suba seus bins, e importe eles. Para importa todo o set de bins mais facilmente, procure nas apps *Batch Import Assembly from Staging Area*. Desse jeito todos seus genomas ser√£o subidos na narrativa criada e estar√£o dispon√≠veis para as an√°lises seguintes. √â importante que na descri√ß√£o do tipo de arquivo que est√° subindo, coloque que se trata de MAGs.

Ap√≥s carregados todos os genomas na plataforma, procure no men√∫ de apps *GTDB-Tk classify*, selecione o set de bins carregados e d√™ click em Run, aguarde at√© o resultado estiver pronto e descarregue a tabela `.csv` do reporte. A tabela √© um pouco bagun√ßada, mas use o Excel para ajeitar, de maneira que fique s√≥ as colunas mais importantes, tais como *Genoma, Clasifica√ß√£o taxon√¥mica* (aqui √© bom separar em colunas, por cada n√≠vel taxon√¥mico). Use este [script](https://github.com/khidalgo85/Binning/blob/master/taxonomy.R) para juntar esta tabela, com a criada no [script da sa√≠da do Magpurify](https://github.com/khidalgo85/Binning/blob/master/binning.R) usado para compara√ß√£o do antes e depois de us√°-lo, e gerar uma grande tabela com as informa√ß√µes mais relavantes do processo de binning.

```{r,echo=FALSE, message=FALSE, comment=FALSE}
library(dplyr)
library(kableExtra)

tabela <- read.csv2("tabela_final_binning.csv", sep = ",")

df<- tabela
color.me <- which(df$Inicial.MiMAG.Quality == "High-Quality")

color.me1 <- which(df$Inicial.MiMAG.Quality == "Medium-Quality")

color.me2 <- which(df$Inicial.MiMAG.Quality == "Low-Quality")

df %>% 
  kable(escape = F, full_width = FALSE) %>%
  kable_styling() %>%
  row_spec(color.me, bold = T, color = "white", background = "green") %>% 
  kable_styling() %>%
  row_spec(color.me1, bold = T, color = "white", background = "orange") %>% 
  kable_styling() %>%
  row_spec(color.me2, bold = T, color = "white", background = "red") %>% 
      row_spec(0, background = "rgb(172, 178, 152)", color = "black", font_size = 18)
```



### 11. Abund√¢ncia relativa dos MAGs nas amostras

Usando o programa **CoverM** √© poss√≠vel calcular a abund√¢ncia relativa de cada MAG em cada uma das amostras. Para isto √© necess√°rio usar os arquivos `.sorted.bam` para mapear os genomas dentros das reads das amostras.

A continua√ß√£o, nos gr√°ficos de barras pode ser observada a abund√¢ncia relativa dos  MAGs em cada amostra, junto com a afilia√ß√£o taxon√¥mica no n√≠vel de Filum (A) e de Familia (B). Para construir esses gr√°ficos foi usado este [script]()

```{r, echo=FALSE, message=FALSE}


## Gr√°fico da abundancia relativa dos MAGs nas amostras
library(dplyr)
t <- read.delim("docs/coverm.tsv", sep = "\t") %>%
  filter(Genome != "binsanity_-kmean-bin_74_sub")

y <- read.csv2("tabela_final_binning.csv", sep = ",") %>%
  rbind(c("unmapped"))

x <- merge(y, t, by = "Genome")

x1 <- rename(x, Sample.1 = B52, Sample.2 = B63, Sample.3 = B65, Sample.4 = PM62, Sample.5 = PM63, Sample.6 = PM65)

library(tidyr)

x2 <- x1 %>%
  gather(Sample, RelativeAbundance, Sample.1:Sample.6)
x2 <- as_tibble(x2)


mypalette <- c("#50a4d3",
               "#45c3c6",
               "#5567a8",
               "#af9b54",
               "#4fb187",
               "#8798e2",
               "#4e7330",
               "#e48c72",
               "#af4f5f",
               "#a05185",
               "#98602a",
               "#db8bc7",
               "#81b263",
               "#8a53a7",
               "#d99833",
               "#db457a",
               "#5b6dd9",
               "#a0b434",
               "#d04c34",
               "#51b74c",
               "#ce4eb4",
               "#a360d8")

library(ggplot2)
### Phylum
p <- ggplot(x2, aes(y=Sample, x=RelativeAbundance, fill=Phylum)) + 
  geom_bar(stat='identity') + scale_fill_manual(values=c(mypalette)) +
  scale_y_discrete(labels=c("B52", "B63", "B65", "PM62", "PM63", "PM65"))

### Family

p1 <- ggplot(x2, aes(y=Sample, x=RelativeAbundance, fill=Family)) + 
  geom_bar(stat='identity') + scale_fill_manual(values=c(mypalette)) +
  scale_y_discrete(labels=c("B52", "B63", "B65", "PM62", "PM63", "PM65"))


### Both
library(ggpubr)
pf <- ggarrange(p, p1, labels = c("A", "B"),
                ncol = 1, nrow = 2)
pf

```


### 12. Anota√ß√£o funcional


Em constru√ß√£o...
